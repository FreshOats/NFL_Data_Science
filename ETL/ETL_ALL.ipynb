{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETL for both Concussion and Surface Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import sqlalchemy as db\n",
    "from sqlalchemy.orm import Session\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "\n",
    "from CleaningFunctions import *\n",
    "from TransformFunctions import *\n",
    "from DataHandler import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_db_connection():\n",
    "    from config import db_password\n",
    "    # uri = f\"postgresql://postgres:{db_password}@127.0.0.1:5432/{database}\"\n",
    "    \n",
    "    return psycopg2.connect(\n",
    "        dbname='nfl_surface',\n",
    "        user='postgres',\n",
    "        password=db_password,\n",
    "        host='127.0.0.1',\n",
    "        port='5432'\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def fetch_data_chunks(connection, chunk_size=10000):\n",
    "    with connection.cursor() as cursor:\n",
    "        cursor.execute(\"\"\"\n",
    "            SELECT DISTINCT playkey \n",
    "            FROM tracking \n",
    "            ORDER BY playkey\n",
    "        \"\"\")\n",
    "        \n",
    "        while True:\n",
    "            playkeys = cursor.fetchmany(chunk_size)\n",
    "            if not playkeys:\n",
    "                break\n",
    "            \n",
    "            playkey_list = [pk[0] for pk in playkeys]\n",
    "            \n",
    "            chunk_query = \"\"\"\n",
    "                SELECT * FROM tracking\n",
    "                WHERE playkey IN %s\n",
    "                ORDER BY playkey\n",
    "            \"\"\"\n",
    "            cursor.execute(chunk_query, (tuple(playkey_list),))\n",
    "            \n",
    "            # Fetch the results and create a Polars DataFrame\n",
    "            chunk_data = cursor.fetchall()\n",
    "            column_names = [desc[0] for desc in cursor.description]\n",
    "            chunk_df = pl.DataFrame(chunk_data, schema=column_names)\n",
    "            \n",
    "            yield chunk_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data():\n",
    "    connection = get_db_connection()\n",
    "\n",
    "    for chunk in fetch_data_chunks(connection):\n",
    "        # Process each chunk\n",
    "        processed_chunk = process_chunk(chunk)\n",
    "        \n",
    "        # Do something with the processed chunk (e.g., save to file, further analysis)\n",
    "        save_or_analyze(processed_chunk)\n",
    "\n",
    "    connection.close()\n",
    "\n",
    "def process_chunk(chunk):\n",
    "    # Apply your processing functions here\n",
    "    chunk = angle_corrector(chunk)\n",
    "    chunk = velocity_calculator(chunk)\n",
    "    # Add more processing as needed\n",
    "    return chunk\n",
    "\n",
    "def save_or_analyze(processed_chunk):\n",
    "    # Implement your saving or further analysis logic here\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = process_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "import polars as pl\n",
    "\n",
    "def set_optimal_chunk_size():\n",
    "    \"\"\"\n",
    "    The data is in the range of GB, so it's critical to chunk and determine optimal\n",
    "    chunk size. A further issue is ensuring each PlayKey is not broken up. \n",
    "    \"\"\"\n",
    "    import psutil\n",
    "    import polars as pl\n",
    "    # Get available memory in bytes\n",
    "    available_memory = psutil.virtual_memory().available\n",
    "    \n",
    "    # Convert to GB for easier understanding\n",
    "    available_memory_gb = available_memory / (1024 ** 3)\n",
    "    \n",
    "    # Set chunk size to 10% of available memory, with a minimum of 1000 rows\n",
    "    # and a maximum of 100,000 rows\n",
    "    chunk_size = max(1000, min(100000, int(available_memory_gb * 0.1 * 1e6)))\n",
    "    \n",
    "    print(f\"Available memory: {available_memory_gb:.2f} GB\")\n",
    "    print(f\"Setting chunk size to: {chunk_size} rows\")\n",
    "    \n",
    "    # Set the streaming chunk size in Polars\n",
    "    pl.Config.set_streaming_chunk_size(chunk_size)\n",
    "\n",
    "# Call this function before processing your data\n",
    "chunk_size = set_optimal_chunk_size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_by_playkey_chunks(lazy_df, chunk_size=chunk_size):\n",
    "    \"\"\"\n",
    "    This ensures that when defining chunks that the PlayKeys are not broken up.\n",
    "    \"\"\"\n",
    "    import polars as pl\n",
    "    # Get unique PlayKeys\n",
    "    unique_playkeys = lazy_df.select(pl.col(\"PlayKey\").unique()).collect()\n",
    "    \n",
    "    results = []\n",
    "    for i in range(0, len(unique_playkeys), chunk_size):\n",
    "        chunk_playkeys = unique_playkeys[i:i+chunk_size]\n",
    "        \n",
    "        # Filter the LazyFrame for the current chunk of PlayKeys\n",
    "        chunk_df = lazy_df.filter(pl.col(\"PlayKey\").is_in(chunk_playkeys[\"PlayKey\"]))\n",
    "        \n",
    "        # Process the chunk (replace this with your actual processing logic)\n",
    "        processed_chunk = process_chunk(chunk_df)\n",
    "        \n",
    "        results.append(processed_chunk)\n",
    "        \n",
    "        print(f\"Processed chunk {i//chunk_size + 1} of {(len(unique_playkeys)-1)//chunk_size + 1}\")\n",
    "    \n",
    "    # Combine results if needed\n",
    "    return pl.concat(results)\n",
    "\n",
    "def process_chunk(chunk_df):\n",
    "    chunk_df = angle_corrector(chunk_df).drop('event')\n",
    "    chunk_df = velocity_calculator(chunk_df)\n",
    "    chunk_df = body_builder(chunk_df)\n",
    "    chunk_df = impulse_calculator(chunk_df)\n",
    "    \n",
    "\n",
    "    # This function should return a DataFrame or LazyFrame\n",
    "    return chunk_df.collect()\n",
    "\n",
    "# Assuming you have a LazyFrame called 'lazy_df'\n",
    "# result = process_by_playkey_chunks(lazy_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lazy_df = data_loader(dataset='tracking', database='nfl_surface')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = process_by_playkey_chunks(lazy_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the set of functions from the concussion sets. Most of these overlap with the surface injury sets.\n",
    "\n",
    "## Injury Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_injuries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TransformFunctions import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm running into issues with memory using the Bodybuilder function, which is pulling data from the database. I think it will be more efficient to perform these joins prior to opening the tracking data, and saving them to the local file. Then when needed, I can lazy load these and do the joins with the large table using lazyframes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def body_builder(chunk_df):\n",
    "    import polars as pl # type: ignore\n",
    "    from DataHandler import data_loader\n",
    "\n",
    "    body_data = pl.LazyFrame({\n",
    "        \"position\": [\"QB\", \"RB\", \"FB\", \"WR\", \"TE\", \"T\", \"G\", \"C\", \"DE\", \"DT\", \"NT\", \"LB\", \"OLB\", \"MLB\", \"CB\", \"S\", \"K\", \"P\", \"SS\", \"ILB\", \"FS\", \"LS\", \"DB\"]\n",
    "        # , \"Position_Name\": [\"Quarterback\", \"Running Back\", \"Fullback\", \"Wide Receiver\", \"Tight End\", \"Tackle\", \"Guard\", \"Center\", \"Defensive End\", \"Defensive Tackle\", \"Nose Tackle\", \"Linebacker\", \"Outside Linebacker\", \"Middle Linebacker\", \"Cornerback\", \"Safety\", \"Kicker\", \"Punter\", \"Strong Safety\", \"Inside Linebacker\", \"Free Safety\", \"Long Snapper\", \"Defensive Back\"]\n",
    "        , \"Height_m\": [1.91, 1.79, 1.85, 1.88, 1.96, 1.97, 1.90, 1.87, 1.97, 1.92, 1.88, 1.90, 1.90, 1.87, 1.82, 1.84, 1.83, 1.88, 1.84, 1.90, 1.84, 1.88, 1.82]\n",
    "        , \"Weight_kg\": [102.1, 95.3, 111.1, 90.7, 114.6, 140.6, 141.8, 136.1, 120.2, 141.8, 152.0, 110.0, 108.9, 113.4, 87.4, 95.9, 92.08, 97.52, 95.9, 110.0, 95.9, 108.86, 87.4]\n",
    "        , \"Chest_rad_m\": [0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191]\n",
    "        })\n",
    "\n",
    "    position = data_loader(dataset='play_positions', database='nfl_surface')\n",
    "    position = position.lazy().join(\n",
    "                    body_data\n",
    "                    , on='position'\n",
    "                    , how='left'\n",
    "                    )\n",
    "\n",
    "    del body_data\n",
    "\n",
    "    chunk_df = chunk_df.join(\n",
    "                position\n",
    "                , left_on='PlayKey'\n",
    "                , right_on='playkey'\n",
    "                , how='left'\n",
    "            )  \n",
    "    \n",
    "    return chunk_df\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan = data_loader(dataset='tracking', database='nfl_surface')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan = angle_corrector(scan).drop('event')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan = velocity_calculator(scan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant = angle_corrector(quant).drop('event')\n",
    "quant = velocity_calculator(quant)\n",
    "quant = impulse_calculator(quant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant = quant.head(1000).collect(streaming=True, infer_schema_length=10000)\n",
    "quant.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concussion Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_concussions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_concussion_data('summary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_concussion_data('tracking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cleaning Functions\n",
    "# # This will set up the code for all cleaning functions. Currently, this cleans only the injury data for surface injuries. \n",
    "# # TO RUN: \n",
    "#     # from CleaningFunctions import *\n",
    "#     # clean_injuries()\n",
    "\n",
    "\n",
    "# ##### Primary Cleaning Functions #####\n",
    "# def clean_injuries():\n",
    "#     from DataHandler import data_writer\n",
    "#     database = \"nfl_surface\"\n",
    "#     quals = table_joiner() \n",
    "#     quals = column_capitalizer(quals, df_name='quals')\n",
    "#     quals = stadium_cleaner(quals, df_name='quals')\n",
    "#     quals = weather_cleaner(quals)\n",
    "#     quals = injury_cleaner(quals)\n",
    "#     data_writer(quals, database, \"qualitative\")\n",
    "#     print(\"Play and Injury Data has been cleaned and uploaded as qualitative\")\n",
    "#     del quals\n",
    "\n",
    "\n",
    "# def clean_concussions():\n",
    "#     from DataHandler import data_loader, data_writer\n",
    "#     import polars as pl\n",
    "#     database = 'nfl_concussion'\n",
    "\n",
    "#     df = data_loader(database='nfl_concussion', dataset='concussion')\n",
    "#     df = column_capitalizer(df, 'concussion')\n",
    "#     df = stadium_cleaner(df, 'concussion')\n",
    "#     df = weather_cleaner(df)\n",
    "#     df = turf_cleaner(df)\n",
    "#     df = df.filter(pl.col(\"Game_Date\").is_not_null())\n",
    "#     df = score_splitter(df)\n",
    "#     data_writer(df, database, \"clean_data\")\n",
    "#     print(\"Concussion Data has been cleaned and uploaded as clean_data\")\n",
    "#     # return df\n",
    "\n",
    "\n",
    "\n",
    "# ######################################################################################\n",
    "# # Extracts and joins the necessary columns from the Injuries and Plays tables\n",
    "# def table_joiner():\n",
    "#     import polars as pl\n",
    "#     from DataHandler import data_loader\n",
    "\n",
    "#     plays = data_loader('plays', 'nfl_surface')\n",
    "#     injuries = data_loader('injuries', 'nfl_surface')\n",
    "\n",
    "#     quals = (\n",
    "#         plays.join(injuries, on=\"playkey\", how='left')\n",
    "#         .select([\n",
    "#             pl.col(\"playkey\")\n",
    "#             , pl.col(\"rosterposition\")\n",
    "#             , pl.col(\"stadiumtype\")\n",
    "#             , pl.col(\"fieldtype\")\n",
    "#             , pl.col(\"temperature\")\n",
    "#             , pl.col(\"weather\")\n",
    "#             , pl.col(\"playtype\")\n",
    "#             , pl.col(\"bodypart\")\n",
    "#             , pl.col(\"dm_m1\")\n",
    "#             , pl.col(\"dm_m7\")\n",
    "#             , pl.col(\"dm_m28\")\n",
    "#             , pl.col(\"dm_m42\")\n",
    "\n",
    "#         ])\n",
    "#     )\n",
    "#     return quals\n",
    "\n",
    "\n",
    "# # Changes the all lower-case to Capitalized PascalCase column headers \n",
    "# def column_capitalizer(df, df_name):          \n",
    "#     if df_name == 'quals':\n",
    "#         columns = {\n",
    "#         'playkey': \"PlayKey\"\n",
    "#         , 'position': 'Position'\n",
    "#         , 'stadiumtype': 'Stadium_Type'\n",
    "#         , 'fieldtype': 'Field_Type'\n",
    "#         , 'temperature': 'Temperature'\n",
    "#         , 'weather': 'Weather'\n",
    "#         , 'playtype': 'Play_Type'\n",
    "#         , 'bodypart': 'Body_Part'\n",
    "#         , 'dm_m1': 'DM_1'\n",
    "#         , 'dm_m7': 'DM_7'\n",
    "#         , 'dm_m28': 'DM_28'\n",
    "#         , 'dm_m42': 'DM_42'\n",
    "#         }\n",
    "\n",
    "#     elif df_name == 'concussion':\n",
    "#         columns = {\n",
    "#         'playkey': 'Playkey'\n",
    "#         , 'position': 'Position'\n",
    "#         , 'role': 'Role'\n",
    "#         , 'game_date': 'Game_Date'\n",
    "#         , 'yardline': 'Yardline'\n",
    "#         , 'quarter': 'Quarter'\n",
    "#         , 'play_type': 'Play_Type'\n",
    "#         , 'poss_team': 'Poss_Team'\n",
    "#         , 'score_home_visiting': 'Score_Home_Visiting'\n",
    "#         , 'game_site': 'Game_Site'\n",
    "#         , 'start_time': 'Start_Time'\n",
    "#         , 'hometeamcode': 'Home_Team_Code'\n",
    "#         , 'visitteamcode': 'Visit_Team_Code'\n",
    "#         , 'stadiumtype': 'Stadium_Type'\n",
    "#         , 'turf': 'Field_Type'\n",
    "#         , 'gameweather': 'Weather'\n",
    "#         , 'temperature': 'Temperature'\n",
    "#         , 'player_activity_derived': 'Player_Activity_Derived'\n",
    "#         , 'primary_impact_type': 'Primary_Impact_Type'\n",
    "#         , 'primary_partner_activity_derived': 'Primary_Partner_Activity_Derived'\n",
    "#         , 'primary_partner_gsisid': 'Primary_Partner_Gsisid'\n",
    "#         }\n",
    "\n",
    "\n",
    "#     df = df.rename(columns)\n",
    "#     return df\n",
    "\n",
    "# # This changes stadiums to either Indoor or Outdoor per game records - some of the dome stadiums have a roof that can open, if open the game is considered outdoor.\n",
    "# def stadium_cleaner(df, df_name):\n",
    "#     import polars as pl \n",
    "\n",
    "#     if df_name == 'quals':       \n",
    "#         stadium_dict = {\n",
    "#             'Outdoor': 'Outdoor'\n",
    "#             , 'Indoors': 'Indoor'\n",
    "#             , 'Oudoor': 'Outdoor'\n",
    "#             , 'Outdoors': 'Outdoor'\n",
    "#             , 'Open': 'Outdoor'\n",
    "#             , 'Closed Dome': 'Indoor'\n",
    "#             , 'Domed, closed': 'Indoor'\n",
    "#             , 'Dome': 'Indoor'\n",
    "#             , 'Indoor': 'Indoor'\n",
    "#             , 'Domed': 'Indoor'\n",
    "#             , 'Retr. Roof-Closed': 'Indoor'\n",
    "#             , 'Outdoor Retr Roof-Open': 'Outdoor'\n",
    "#             , 'Retractable Roof': 'Indoor'\n",
    "#             , 'Ourdoor': 'Outdoor'\n",
    "#             , 'Indoor, Roof Closed': 'Indoor'\n",
    "#             , 'Retr. Roof - Closed': 'Indoor'\n",
    "#             , 'Bowl': 'Outdoor'\n",
    "#             , 'Outddors': 'Outdoor'\n",
    "#             , 'Retr. Roof-Open': 'Outdoor'\n",
    "#             , 'Dome, closed': 'Indoor'\n",
    "#             , 'Indoor, Open Roof': 'Outdoor'\n",
    "#             , 'Domed, Open': 'Outdoor'\n",
    "#             , 'Domed, open': 'Outdoor'\n",
    "#             , 'Heinz Field': 'Outdoor'\n",
    "#             , 'Cloudy': 'Outdoor'\n",
    "#             , 'Retr. Roof - Open': 'Outdoor'\n",
    "#             , 'Retr. Roof Closed': 'Indoor'\n",
    "#             , 'Outdor': 'Outdoor'\n",
    "#             , 'Outside': 'Outdoor'\n",
    "#         }\n",
    "\n",
    "\n",
    "#         df = df.with_columns(pl.col(\"Stadium_Type\").fill_null(\"Outdoor\")) # Since most stadiums are outdoor and the percentage of games played indoor is already met by the known indoor games those seasons, all unknown games were set to outdoor\n",
    "\n",
    "\n",
    "#     elif df_name == 'concussion':\n",
    "#         stadium_dict = {\n",
    "#             'Outdoor': 'Outdoor'\n",
    "#             , 'outdoor': 'Outdoor'\n",
    "#             , 'Indoors': 'Indoor'\n",
    "#             , 'Indoors (Domed)': 'Indoor'\n",
    "#             , 'Oudoor': 'Outdoor'\n",
    "#             , 'Outdoors': 'Outdoor'\n",
    "#             , 'Outdoors ': 'Outdoor'\n",
    "#             , 'Open': 'Outdoor'\n",
    "#             , 'Closed Dome': 'Indoor'\n",
    "#             , 'Domed, closed': 'Indoor'\n",
    "#             , 'Dome': 'Indoor'\n",
    "#             , 'Indoor': 'Indoor'\n",
    "#             , 'Domed': 'Indoor'\n",
    "#             , 'Retr. Roof-Closed': 'Indoor'\n",
    "#             , 'Outdoor Retr Roof-Open': 'Outdoor'\n",
    "#             , 'Retractable Roof': 'Indoor'\n",
    "#             , 'Ourdoor': 'Outdoor'\n",
    "#             , 'Indoor, Roof Closed': 'Indoor'\n",
    "#             , 'Retr. Roof - Closed': 'Indoor'\n",
    "#             , 'Bowl': 'Outdoor'\n",
    "#             , 'Outddors': 'Outdoor'\n",
    "#             , 'Retr. Roof-Open': 'Outdoor'\n",
    "#             , 'Dome, closed': 'Indoor'\n",
    "#             , 'Indoor, Open Roof': 'Outdoor'\n",
    "#             , 'Domed, Open': 'Outdoor'\n",
    "#             , 'Domed, open': 'Outdoor'\n",
    "#             , 'Heinz Field': 'Outdoor'\n",
    "#             , 'Cloudy': 'Outdoor'\n",
    "#             , 'Retr. Roof - Open': 'Outdoor'\n",
    "#             , 'Retr. Roof Closed': 'Indoor'\n",
    "#             , 'Outdor': 'Outdoor'\n",
    "#             , 'Outside': 'Outdoor'\n",
    "#             , 'Indoor, non-retractable roof': 'Indoor'\n",
    "#             , 'Retr. roof - closed': 'Indoor'\n",
    "#             , 'Indoor, fixed roof ': 'Indoor'\n",
    "#             , 'Indoor, Non-Retractable Dome': 'Indoor'\n",
    "#             , 'Indoor, Fixed Roof': 'Indoor'\n",
    "#             , 'Indoor, fixed roof': 'Indoor'\n",
    "#             , None: 'Outdoor'  # It was verified that all fields with null values are indeed outdoor\n",
    "#         }\n",
    "\n",
    "\n",
    "#     df = df.with_columns(pl.col(\"Stadium_Type\").replace(stadium_dict)) # This uses the dict to assign naming conventions\n",
    "\n",
    "#     return df\n",
    "\n",
    "# # Cleans up the weather data from having a lot of different but similar to a few categories\n",
    "# def weather_cleaner(df):\n",
    "#      import polars as pl\n",
    "     \n",
    "#      weather_dict = {\n",
    "#             'Clear and warm': 'Clear'\n",
    "#             , 'Mostly Cloudy': 'Cloudy'\n",
    "#             , 'Sunny': 'Clear'\n",
    "#             , 'Clear': 'Clear'\n",
    "#             , 'Cloudy': 'Cloudy'\n",
    "#             , 'Cloudy, fog started developing in 2nd quarter': 'Hazy/Fog'\n",
    "#             , 'Rain': 'Rain'\n",
    "#             , 'Partly Cloudy': 'Cloudy'\n",
    "#             , 'Mostly cloudy': 'Cloudy'\n",
    "#             , 'Cloudy and cold': 'Cloudy'\n",
    "#             , 'Cloudy and Cool': 'Cloudy'\n",
    "#             , 'Rain Chance 40%': 'Rain'\n",
    "#             , 'Controlled Climate': 'Indoor'\n",
    "#             , 'Sunny and warm': 'Clear'\n",
    "#             , 'Partly cloudy': 'Cloudy'\n",
    "#             , 'Clear and Cool': 'Cloudy'\n",
    "#             , 'Clear and cold': 'Cloudy'\n",
    "#             , 'Sunny and cold': 'Clear'\n",
    "#             , 'Indoor': 'Indoor'\n",
    "#             , 'Partly Sunny': 'Clear'\n",
    "#             , 'N/A (Indoors)': 'Indoor'\n",
    "#             , 'Mostly Sunny': 'Clear'\n",
    "#             , 'Indoors': 'Indoor'\n",
    "#             , 'Clear Skies': 'Clear'\n",
    "#             , 'Partly sunny': 'Clear'\n",
    "#             , 'Showers': 'Rain'\n",
    "#             , 'N/A Indoor': 'Indoor'\n",
    "#             , 'Sunny and clear': 'Clear'\n",
    "#             , 'Snow': 'Snow'\n",
    "#             , 'Scattered Showers': 'Rain'\n",
    "#             , 'Party Cloudy': 'Cloudy'\n",
    "#             , 'Clear skies': 'Clear'\n",
    "#             , 'Rain likely, temps in low 40s.': 'Rain'\n",
    "#             , 'Hazy': 'Hazy/Fog'\n",
    "#             , 'Partly Clouidy': 'Cloudy'\n",
    "#             , 'Sunny Skies': 'Clear'\n",
    "#             , 'Overcast': 'Cloudy'\n",
    "#             , 'Cloudy, 50% change of rain': 'Cloudy'\n",
    "#             , 'Fair': 'Clear'\n",
    "#             , 'Light Rain': 'Rain'\n",
    "#             , 'Partly clear': 'Clear'\n",
    "#             , 'Mostly Coudy': 'Cloudy'\n",
    "#             , '10% Chance of Rain': 'Cloudy'\n",
    "#             , 'Cloudy, chance of rain': 'Cloudy'\n",
    "#             , 'Heat Index 95': 'Clear'\n",
    "#             , 'Sunny, highs to upper 80s': 'Clear'\n",
    "#             , 'Sun & clouds': 'Cloudy'\n",
    "#             , 'Heavy lake effect snow': 'Snow'\n",
    "#             , 'Mostly sunny': 'Clear'\n",
    "#             , 'Cloudy, Rain': 'Rain'\n",
    "#             , 'Sunny, Windy': 'Windy'\n",
    "#             , 'Mostly Sunny Skies': 'Clear'\n",
    "#             , 'Rainy': 'Rain'\n",
    "#             , '30% Chance of Rain': 'Rain'\n",
    "#             , 'Cloudy, light snow accumulating 1-3\"': 'Snow'\n",
    "#             , 'cloudy': 'Cloudy'\n",
    "#             , 'Clear and Sunny': 'Clear'\n",
    "#             , 'Coudy': 'Cloudy'\n",
    "#             , 'Clear and sunny': 'Clear'\n",
    "#             , 'Clear to Partly Cloudy': 'Clear'\n",
    "#             , 'Cloudy with periods of rain, thunder possible. Winds shifting to WNW, 10-20 mph.': 'Windy'\n",
    "#             , 'Rain shower': 'Rain'\n",
    "#             , 'Cold': 'Clear'\n",
    "#             , 'Partly cloudy, lows to upper 50s.': 'Cloudy'\n",
    "#             , 'Scattered thunderstorms': 'Rain'\n",
    "#             , 'CLEAR': 'Clear'\n",
    "#             , 'Partly CLoudy': 'Cloudy'\n",
    "#             , 'Chance of Showers': 'Rain'\n",
    "#             , 'Snow showers': 'Snow'\n",
    "#             , 'Clear and Cold': 'Clear'\n",
    "#             , 'Cloudy with rain': 'Rain'\n",
    "#             , 'Sunny intervals': 'Clear'\n",
    "#             , 'Clear and cool': 'Clear'\n",
    "#             , 'Cloudy, Humid, Chance of Rain': 'Rain'\n",
    "#             , 'Cloudy and Cold': 'Cloudy'\n",
    "#             , 'Cloudy with patches of fog': 'Hazy/Fog'\n",
    "#             , 'Controlled': 'Indoor'\n",
    "#             , 'Sunny and Clear': 'Clear'\n",
    "#             , 'Cloudy with Possible Stray Showers/Thundershowers': 'Rain'\n",
    "#             , 'Suny': 'Clear'\n",
    "#             , 'T-Storms': 'Rain'\n",
    "#             , 'Sunny and cool': 'Clear'\n",
    "#             , 'Cloudy, steady temps': 'Cloudy'\n",
    "#             , 'Hazy, hot and humid': 'Hazy/Fog'\n",
    "#             , 'Sunny Intervals': 'Clear'\n",
    "#             , 'Partly Cloudy, Chance of Rain 80%': 'Rain'\n",
    "#             , 'Mostly Clear. Gusting ot 14.': 'Windy'\n",
    "#             , 'Mostly CLoudy': 'Cloudy'\n",
    "#             , 'Snow Showers, 3 to 5 inches expected.': 'Snow'\n",
    "#             }\n",
    "\n",
    "\n",
    "#      df = df.with_columns(pl.col(\"Weather\").replace(weather_dict)) # Standardizes the weather to a few main types\n",
    "\n",
    "#      df = df.with_columns(             # Null handling - all null weather conditions for indoor stadiums are filled \"indoor\"\n",
    "#                 pl.when(pl.col(\"Stadium_Type\") == \"Indoor\")\n",
    "#                 .then(pl.col(\"Weather\").fill_null(\"Indoor\"))\n",
    "#                 .otherwise(pl.col(\"Weather\"))\n",
    "#                 .alias(\"Weather\")\n",
    "#                 )\n",
    "     \n",
    "#      # For the non-indoor games with null values for weather, to maintain the percentage of games that were clear/cloudy, temperature was used as a divider, above and below 70 degrees\n",
    "#      df = df.with_columns(\n",
    "#                 pl.when(pl.col(\"Temperature\") > 70)\n",
    "#                 .then(pl.col(\"Weather\").fill_null(\"Clear\"))\n",
    "#                 .otherwise(pl.col(\"Weather\"))\n",
    "#                 .alias(\"Weather\")\n",
    "#                 )\n",
    "#      df = df.with_columns(pl.col(\"Weather\").fill_null(\"Cloudy\"))\n",
    "\n",
    "#      return df\n",
    "\n",
    "\n",
    "# # This fixes the issues with introduced nulls following the joins \n",
    "# def injury_cleaner(quals):\n",
    "#     import polars as pl\n",
    "#     quals = quals.filter(pl.col('Play_Type').is_not_null()) # 0.14% of rows did not have a play type, and ALL of these were non-injury plays, so they were removed\n",
    "\n",
    "#     quals = quals.with_columns(pl.col(\"Body_Part\").fill_null(\"No_Injury\")) # This fills all null from the join with No Injury\n",
    "\n",
    "#     quals = quals.with_columns(\n",
    "#     pl.col([\"DM_1\", \"DM_7\", \"DM_28\", \"DM_42\"]).fill_null(0)) # This fills the nulls from the Join with 0s, since there were no injuries.\n",
    "\n",
    "#     return quals\n",
    "\n",
    "\n",
    "# # This will standardize the types of Turf for the FieldType to either natural or synthetic\n",
    "\n",
    "# def turf_cleaner(df):\n",
    "#     ''' \n",
    "#     Changes the many different types of turf listed into either natural or synthetic\n",
    "#     '''\n",
    "#     import polars as pl\n",
    "\n",
    "#     turf_dict = {\n",
    "#         'Grass': 'Natural'\n",
    "#         , 'Field Turf': 'Synthetic'\n",
    "#         , 'Natural Grass': 'Natural'\n",
    "#         , 'grass': 'Natural'\n",
    "#         , 'Artificial': 'Synthetic'\n",
    "#         , 'FieldTurf': 'Synthetic'\n",
    "#         , 'DD GrassMaster': 'Synthetic'\n",
    "#         , 'A-Turf Titan': 'Synthetic'\n",
    "#         , 'UBU Sports Speed S5-M': 'Synthetic'\n",
    "#         , 'UBU Speed Series S5-M': 'Synthetic'\n",
    "#         , 'Artifical': 'Synthetic'\n",
    "#         , 'UBU Speed Series-S5-M': 'Synthetic'\n",
    "#         , 'FieldTurf 360': 'Synthetic'\n",
    "#         , 'Natural grass': 'Natural'\n",
    "#         , 'Field turf': 'Synthetic'\n",
    "#         , 'Natural': 'Natural'\n",
    "#         , 'Natrual Grass': 'Natural'\n",
    "#         , 'Synthetic': 'Synthetic'\n",
    "#         , 'Natural Grass ': 'Natural'\n",
    "#         , 'Naturall Grass': 'Natural'\n",
    "#         , 'FieldTurf360': 'Synthetic'\n",
    "#         , None: 'Natural'  # The only field with null values is Miami Gardens, which has Natural\n",
    "#         }\n",
    "\n",
    "    \n",
    "#     df = df.with_columns(pl.col(\"Field_Type\").replace(turf_dict))\n",
    "#     return df\n",
    "\n",
    "# def score_splitter(df):\n",
    "#     ''' \n",
    "#     Splits the string column from Score_Home_Visiting into two numeric columns for each of the scores. It also creates a column that calculates the difference. \n",
    "#     '''\n",
    "#     import polars as pl\n",
    "\n",
    "#     df = df.with_columns([\n",
    "#         pl.col(\"Score_Home_Visiting\").str.extract(r\"(\\d+)\\s*-\\s*(\\d+)\", 1).cast(pl.Int16).alias(\"Home_Score\")\n",
    "#         , pl.col(\"Score_Home_Visiting\").str.extract(r\"(\\d+)\\s*-\\s*(\\d+)\", 2).cast(pl.Int16).alias(\"Visiting_Score\") # Find difference between scores\n",
    "#         ])\n",
    "\n",
    "#     df = df.with_columns([\n",
    "#         (pl.col(\"Home_Score\") - pl.col(\"Visiting_Score\")).cast(pl.Int16).alias(\"Score_Difference\")\n",
    "#         ])\n",
    "    \n",
    "#     df = df.drop(\"Score_Home_Visiting\")\n",
    "    \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def transform_injury_data(output):\n",
    "#     from DataHandler import data_loader, data_shrinker, data_writer\n",
    "\n",
    "#     valid_outputs = ['tracking', 'summary']\n",
    "#     if output not in valid_outputs:\n",
    "#         raise ValueError(f\"Invalid ouptut selection: '{output}'. Valid options are: '{valid_outputs}'\")\n",
    "\n",
    "#     try: \n",
    "#         # Transform the tracking data\n",
    "#         quant = data_loader(dataset='tracking', database='nfl_surface')\n",
    "#         quant = data_shrinker(quant)\n",
    "#         quant = angle_corrector(quant)\n",
    "#         quant = body_builder(quant, 'tracking')\n",
    "#         quant = velocity_calculator(quant)\n",
    "#         quant = impulse_calculator(quant)\n",
    "    \n",
    "#         if output == 'summary':\n",
    "#             summary = path_calculator(quant)\n",
    "#             del quant # remove the large table from memory\n",
    "#             # Open and merge the qualitative data\n",
    "#             quals = data_loader('qualitative', 'nfl_surface')\n",
    "#             qual_quant = qual_quant_merger(quals, summary)\n",
    "            \n",
    "#             print(\"Writing all quantitative and qualitative summary data to the database as summary_data. Wait.\")\n",
    "#             data_writer(qual_quant, 'nfl_surface', 'summary_data')\n",
    "#             print(\"Data has been uploaded to the database. Probably.\")        \n",
    "\n",
    "#         elif output == 'tracking':\n",
    "#             # upload the physical data to the database for machine learning\n",
    "#             print(\"Writing the transformed table with physical parameters to the database as quantitative\")\n",
    "#             data_writer(quant, 'nfl_surface', 'quantitative')\n",
    "#             print(\"Data has been uploaded to the database. Go celebrate!\")\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred with your selection, '{output}': {e}\")\n",
    "#         return None\n",
    "\n",
    "\n",
    "       \n",
    "# def transform_concussion_data(output):\n",
    "#     from DataHandler import data_loader, data_shrinker, data_writer\n",
    "\n",
    "#     valid_outputs = ['tracking', 'summary']\n",
    "#     if output not in valid_outputs:\n",
    "#         raise ValueError(f\"Invalid ouptut selection: '{output}'. Valid options are: '{valid_outputs}'\")\n",
    "\n",
    "#     try: \n",
    "#         track = data_loader(dataset='ngs_data', database='nfl_concussion')\n",
    "#         track = data_shrinker(track)\n",
    "#         track = column_corrector(track)\n",
    "#         track = angle_corrector(track)\n",
    "#         track = body_builder(track, 'ngs_data')\n",
    "#         track = velocity_calculator(track)\n",
    "#         track = impulse_calculator(track)\n",
    "    \n",
    "#         if output == 'summary':\n",
    "#             summary =  path_calculator(track)\n",
    "#             del track # remove the large table from memory\n",
    "#             # Open and merge the qualitative data\n",
    "#             quals = data_loader('qualitative', 'nfl_concussion')\n",
    "#             qual_quant = qual_quant_merger(quals, summary)\n",
    "            \n",
    "#             print(\"Writing all quantitative and qualitative summary data to the database as summary_data. Wait.\")\n",
    "#             data_writer(qual_quant, 'nfl_concussion', 'summary_data')\n",
    "#             print(\"Data has been uploaded to the database. Probably.\")        \n",
    "\n",
    "#         elif output == 'tracking':\n",
    "#             # upload the physical data to the database for machine learning\n",
    "#             print(\"Writing the transformed table with physical parameters to the database as quantitative\")\n",
    "#             data_writer(track, 'nfl_surface', 'quantitative')\n",
    "#             print(\"Data has been uploaded to the database. Good for you!\")\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred with your selection, '{output}': {e}\")\n",
    "#         return None\n",
    "\n",
    "\n",
    "\n",
    "# def transform_concussion_data():\n",
    "#     from DataHandler import data_loader, data_shrinker, data_writer\n",
    "\n",
    "#     track = data_loader(dataset='ngs_data', database='nfl_concussion')\n",
    "#     track = data_shrinker(track)\n",
    "#     track = column_corrector(track)\n",
    "#     track = angle_corrector(track)\n",
    "#     track = body_builder(track, 'ngs_data')\n",
    "#     track = velocity_calculator(track)\n",
    "#     track = impulse_calculator(track)\n",
    "#     summary = path_calculator(track)\n",
    "#     # transform the ngs_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #############################################\n",
    "# def column_corrector(df):\n",
    "#     import polars as pl\n",
    "#     \"\"\"\n",
    "#     Add a Play_Time column that acts like the 'time' column did in the injury dataset. \n",
    "#     Each PlayKey will start at 0.0 and increase by 0.1 for each subsequent record.\n",
    "#     \"\"\"\n",
    "#     df = df.with_columns([\n",
    "#         pl.concat_str([\n",
    "#             pl.col('gsisid').cast(pl.Int32).cast(pl.Utf8)\n",
    "#             , pl.lit('-')\n",
    "#             , pl.col('gamekey').cast(pl.Utf8)\n",
    "#             , pl.lit('-')\n",
    "#             , pl.col('playid').cast(pl.Utf8)\n",
    "#         ]).alias('PlayKey')\n",
    "#     ])\n",
    "     \n",
    "    \n",
    "#     df = df.select([\n",
    "#         'PlayKey'\n",
    "#         , 'time'\n",
    "#         , 'x'\n",
    "#         , 'y'\n",
    "#         , 'o'\n",
    "#         , 'dir'\n",
    "#         , 'gsisid'\n",
    "#         ]).rename({\"time\":\"datetime\"})\n",
    "\n",
    "#     df = df.sort(['PlayKey', 'datetime'])\n",
    "\n",
    "#     df = df.with_columns(\n",
    "#         (pl.arange(0, pl.len()) * 0.1).over(\"PlayKey\").alias(\"time\")\n",
    "#         ).with_columns([pl.col('gsisid').cast(pl.Int32)])  \n",
    "    \n",
    "#     return df\n",
    "\n",
    "\n",
    "\n",
    "# def calculate_angle_difference(angle1, angle2):\n",
    "#     import numpy as np\n",
    "#     \"\"\"\n",
    "#     Calculate the smallest angle difference between two angles \n",
    "#     using trigonometric functions, accounting for edge cases.\n",
    "#     \"\"\"\n",
    "#     sin_diff = np.sin(np.radians(angle2 - angle1))\n",
    "#     cos_diff = np.cos(np.radians(angle2 - angle1))\n",
    "#     return np.degrees(np.arctan2(sin_diff, cos_diff))\n",
    "\n",
    "# def angle_corrector(df):\n",
    "#     import polars as pl\n",
    "#     \"\"\"\n",
    "#     Make corrections to angles to reduce fringe errors at 360\n",
    "#     \"\"\"\n",
    "#     df = df.with_columns([\n",
    "#         ((pl.col(\"dir\") + 180) % 360 - 180).alias(\"dir\")\n",
    "#         , ((pl.col(\"o\") + 180) % 360 - 180).alias(\"o\")\n",
    "#     ]).with_columns(\n",
    "#         (calculate_angle_difference(pl.col(\"dir\"), pl.col(\"o\"))).abs().round(2).alias(\"Angle_Diff\")\n",
    "#         )\n",
    "    \n",
    "#     return df\n",
    "\n",
    "\n",
    "# def body_builder(df, df_name):\n",
    "#     body_data = pl.DataFrame({\n",
    "#         \"position\": [\"QB\", \"RB\", \"FB\", \"WR\", \"TE\", \"T\", \"G\", \"C\", \"DE\", \"DT\", \"NT\", \"LB\", \"OLB\", \"MLB\", \"CB\", \"S\", \"K\", \"P\", \"SS\", \"ILB\", \"FS\", \"LS\", \"DB\"]\n",
    "#         # , \"Position_Name\": [\"Quarterback\", \"Running Back\", \"Fullback\", \"Wide Receiver\", \"Tight End\", \"Tackle\", \"Guard\", \"Center\", \"Defensive End\", \"Defensive Tackle\", \"Nose Tackle\", \"Linebacker\", \"Outside Linebacker\", \"Middle Linebacker\", \"Cornerback\", \"Safety\", \"Kicker\", \"Punter\", \"Strong Safety\", \"Inside Linebacker\", \"Free Safety\", \"Long Snapper\", \"Defensive Back\"]\n",
    "#         , \"Height_m\": [1.91, 1.79, 1.85, 1.88, 1.96, 1.97, 1.90, 1.87, 1.97, 1.92, 1.88, 1.90, 1.90, 1.87, 1.82, 1.84, 1.83, 1.88, 1.84, 1.90, 1.84, 1.88, 1.82]\n",
    "#         , \"Weight_kg\": [102.1, 95.3, 111.1, 90.7, 114.6, 140.6, 141.8, 136.1, 120.2, 141.8, 152.0, 110.0, 108.9, 113.4, 87.4, 95.9, 92.08, 97.52, 95.9, 110.0, 95.9, 108.86, 87.4]\n",
    "#         , \"Chest_rad_m\": [0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191]\n",
    "#         })\n",
    "\n",
    "#     valid_df_names = ['ngs_data', 'tracking']\n",
    "#     if df_name not in valid_df_names:\n",
    "#         raise ValueError(f\"Invalid dataframe name '{df_name}'. Valid options are: {valid_df_names}\")\n",
    "\n",
    "#     try: \n",
    "#         if df_name == 'ngs_data':\n",
    "#             position = data_loader(dataset='positions', database='nfl_concussion')\n",
    "#             position = position.join(\n",
    "#                 body_data\n",
    "#                 , left_on='position'\n",
    "#                 , right_on='position'\n",
    "#                 , how='left'\n",
    "#                 )\n",
    "            \n",
    "#             df = df.join(\n",
    "#                 position\n",
    "#                 , on='gsisid'\n",
    "#                 , how='left'\n",
    "#                 ).drop_nulls(subset=['position'])\n",
    "            \n",
    "\n",
    "#         elif df_name == 'tracking':\n",
    "#             position = data_loader(dataset='play_positions', database='nfl_surface')\n",
    "#             position = position.join(\n",
    "#                 body_data\n",
    "#                 , left_on='position'\n",
    "#                 , right_on='position'\n",
    "#                 , how='left'\n",
    "#                 )\n",
    "\n",
    "#             df = df.join(\n",
    "#                 position\n",
    "#                 , left_on='PlayKey'\n",
    "#                 , right_on='playkey'\n",
    "#                 , how='left'\n",
    "#             ).drop_nulls(subset=['position']).drop(['event'])\n",
    "\n",
    "            \n",
    "\n",
    "#         return df    \n",
    "    \n",
    "#     except Exception as e: \n",
    "#         print(f\"An error occurred while loading the dataframe '{df_name}': {e}\")\n",
    "#         return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def velocity_calculator(df):\n",
    "#     import numpy as np\n",
    "#     import polars as pl\n",
    "#     \"\"\"\n",
    "#     Using the (X,Y) and time columns, perform calculations based on the \n",
    "#     difference between two rows to find displacement, speed, direction \n",
    "#     of motion, velocity in x and y components, and the angular velocities \n",
    "#     of the direction of motion and orientations \n",
    "#     \"\"\"\n",
    "    \n",
    "#     return df.with_columns([\n",
    "#         # Convert 'o' and 'dir' to radians\n",
    "#         (pl.col(\"o\") * np.pi / 180).alias(\"o_rad\"),\n",
    "#         (pl.col(\"dir\") * np.pi / 180).alias(\"dir_rad\")\n",
    "#     ]).with_columns([\n",
    "#         # Pre-calculate shifted values\n",
    "#         pl.col(\"x\").shift(1).over(\"PlayKey\").alias(\"prev_x\")\n",
    "#         , pl.col(\"y\").shift(1).over(\"PlayKey\").alias(\"prev_y\")\n",
    "#         # , pl.col(\"time\").shift(1).over(\"PlayKey\").alias(\"prev_time\")\n",
    "#         , pl.col(\"dir_rad\").shift(1).over(\"PlayKey\").alias(\"prev_dir\")\n",
    "#         , pl.col(\"o_rad\").shift(1).over(\"PlayKey\").alias(\"prev_o\")\n",
    "#     ]).with_columns([\n",
    "#         # Calculate the component displacements \n",
    "#           (pl.col(\"x\") - pl.col(\"prev_x\")).alias(\"dx\")\n",
    "#         , (pl.col(\"y\") - pl.col(\"prev_y\")).alias(\"dy\")\n",
    "#     ]).with_columns([\n",
    "#         # Calculate displacement\n",
    "#         ((pl.col(\"dx\")**2 + pl.col(\"dy\")**2)**0.5).alias(\"Displacement\")\n",
    "#     ]).with_columns([\n",
    "#         # Calculate speed\n",
    "#         (pl.col(\"Displacement\") / 0.1).alias(\"Speed\")\n",
    "#         # Calculate direction\n",
    "#         , (np.degrees(np.arctan2(pl.col(\"dx\"), pl.col(\"dy\")))).alias(\"Direction\")\n",
    "#         # Calculate velocity components\n",
    "#         , (pl.col(\"dx\") / 0.1).alias(\"vx\")\n",
    "#         , (pl.col(\"dy\") / 0.1).alias(\"vy\")\n",
    "#         # Calculate angular velocities\n",
    "#         , ((pl.col(\"dir_rad\") - pl.col(\"prev_dir\")) / 0.1).alias(\"omega_dir\")\n",
    "#         , ((pl.col(\"o_rad\") - pl.col(\"prev_o\")) / 0.1).alias(\"omega_o\")\n",
    "#     ]).with_columns([\n",
    "#         ((pl.col(\"omega_dir\") - pl.col(\"omega_o\")).abs()).alias(\"omega_diff\")\n",
    "#     ]).drop([\n",
    "#         \"prev_x\", \"prev_y\", \"prev_dir\", \"prev_o\", \"dx\", \"dy\", \"o_rad\", \"dir_rad\"\n",
    "#     ])\n",
    "\n",
    "\n",
    "# def impulse_calculator(df):\n",
    "#     import numpy as np\n",
    "#     import polars as pl\n",
    "#     \"\"\"\n",
    "#     Using the (X,Y) and time columns, perform calculations based on the velocities and changes \n",
    "#     in velocites along with player mass to get the momentum and impulse, a measure that can \n",
    "#     be assessed along with medical data related to concussions and injuries\n",
    "#     \"\"\"\n",
    "    \n",
    "#     return df.with_columns([\n",
    "#         # Calculate the linear momentum for each instant\n",
    "#         (pl.col('vx') * pl.col('Weight_kg')).alias('px')\n",
    "#         , (pl.col('vy') * pl.col('Weight_kg')).alias('py')\n",
    "\n",
    "#         # Calculate the moment of inertia of a rotating upright body (1/12 mr^2)\n",
    "#         , (1/12 * pl.col('Weight_kg') * (pl.col('Chest_rad_m')**2)).alias('moment')\n",
    "        \n",
    "#         # Calculate the moment of inertia of the upper body turning upright with respect to waist (70% mass)\n",
    "#         , (1/12 * (pl.col('Weight_kg')*0.7) * (pl.col('Chest_rad_m')**2)).alias('moment_upper')\n",
    "    \n",
    "#     ]).with_columns([\n",
    "#           # Calculate the magnitude of linear momentum\n",
    "#         ((pl.col(\"px\")**2 + pl.col(\"py\")**2)**0.5).alias(\"p_magnitude\")\n",
    "        \n",
    "#         # Calculate the angular momentum for the direction\n",
    "#         , (pl.col('omega_dir')*pl.col('moment')).alias('L_dir')\n",
    "\n",
    "#         # Calculate the angular momentum of the upper body with respect to lower\n",
    "#         , (pl.col('omega_diff')*pl.col('moment_upper')).alias('L_diff')\n",
    "\n",
    "\n",
    "#     ]).with_columns([\n",
    "#         # Pre-calculate shifted values for linear and angular momenta\n",
    "#         pl.col(\"px\").shift(1).over(\"PlayKey\").alias(\"prev_px\")\n",
    "#         , pl.col(\"py\").shift(1).over(\"PlayKey\").alias(\"prev_py\")\n",
    "#         , pl.col(\"L_dir\").shift(1).over(\"PlayKey\").alias(\"prev_L_dir\")\n",
    "#         , pl.col(\"L_diff\").shift(1).over(\"PlayKey\").alias(\"prev_L_diff\")\n",
    "        \n",
    "#     ]).with_columns([\n",
    "#         # Calculate impulse, J, which is the change in linear momentum \n",
    "#         ((pl.col(\"px\") - pl.col(\"prev_px\"))).alias(\"Jx\")\n",
    "#         , ((pl.col(\"py\") - pl.col(\"prev_py\"))).alias(\"Jy\")\n",
    "        \n",
    "#     ]).with_columns([\n",
    "#           # Calculate the magnitude of linear momentum\n",
    "#         ((pl.col(\"Jx\")**2 + pl.col(\"Jy\")**2)**0.5).alias(\"J_magnitude\")\n",
    "\n",
    "#         # Calculate torque as the change in angular momentum L over the change in time\n",
    "#         , (((pl.col(\"L_dir\") - pl.col(\"prev_L_dir\"))) / 0.1).alias(\"torque\")\n",
    "#         , (((pl.col(\"L_diff\") - pl.col(\"prev_L_diff\"))) / 0.1).alias(\"torque_internal\")\n",
    "\n",
    "#     ]).drop([\n",
    "#         \"prev_L_dir\", \"prev_px\", \"prev_py\", \"prev_L_diff\"\n",
    "#     ])\n",
    "\n",
    "\n",
    "# def path_calculator(df):\n",
    "#     import polars as pl\n",
    "#     # This provides a summary table that can be integrated with the qualitative data\n",
    "\n",
    "#     # Calculate total distance and displacement for each PlayKey\n",
    "#     # Calculate total distance and displacement for each PlayKey\n",
    "#     result = df.select([\n",
    "#         \"PlayKey\"\n",
    "#         , pl.col(\"Displacement\").sum().over(\"PlayKey\").alias(\"Distance\")\n",
    "#         , pl.col(\"x\").first().over(\"PlayKey\").alias(\"start_x\")\n",
    "#         , pl.col(\"y\").first().over(\"PlayKey\").alias(\"start_y\")\n",
    "#         , pl.col(\"x\").last().over(\"PlayKey\").alias(\"end_x\")\n",
    "#         , pl.col(\"y\").last().over(\"PlayKey\").alias(\"end_y\")\n",
    "#         , pl.col(\"Angle_Diff\").max().over(\"PlayKey\").alias(\"Max_Angle_Diff\")\n",
    "#         , pl.col(\"Angle_Diff\").mean().over(\"PlayKey\").alias(\"Mean_Angle_Diff\")\n",
    "#         , pl.col(\"Speed\").max().over(\"PlayKey\").alias(\"Max_Speed\")\n",
    "#         , pl.col(\"Speed\").mean().over(\"PlayKey\").alias(\"Mean_Speed\")\n",
    "#         , pl.col(\"J_magnitude\").max().over(\"PlayKey\").alias(\"Max_Impulse\")\n",
    "#         , pl.col(\"J_magnitude\").mean().over(\"PlayKey\").alias(\"Mean_Impulse\")\n",
    "#         , pl.col(\"torque\").max().over(\"PlayKey\").alias(\"Max_Torque\")\n",
    "#         , pl.col(\"torque\").mean().over(\"PlayKey\").alias(\"Mean_Torque\")\n",
    "#         , pl.col(\"torque_internal\").max().over(\"PlayKey\").alias(\"Max_Int_Torque\")\n",
    "#         , pl.col(\"torque_internal\").mean().over(\"PlayKey\").alias(\"Mean_Int_Torque\")\n",
    "\n",
    "#         ]).unique(subset=[\"PlayKey\"])\n",
    "\n",
    "\n",
    "#     # Calculate the displacement\n",
    "#     result = result.with_columns([\n",
    "#         (((pl.col(\"end_x\") - pl.col(\"start_x\"))**2 + \n",
    "#           (pl.col(\"end_y\") - pl.col(\"start_y\"))**2)**0.5)\n",
    "#         .alias(\"Displacement\")\n",
    "#         ]).with_columns([\n",
    "#             (pl.col(\"Distance\") - pl.col(\"Displacement\")).alias(\"Path_Diff\")\n",
    "#         ])\n",
    "\n",
    "     \n",
    "#     # Select only the required columns\n",
    "#     result = result.select([\n",
    "#         'PlayKey'\n",
    "#         , 'Distance'\n",
    "#         , 'Displacement'\n",
    "#         , 'Path_Diff'\n",
    "#         , 'Max_Angle_Diff'\n",
    "#         , 'Mean_Angle_Diff'\n",
    "#         , 'Max_Speed'\n",
    "#         , 'Mean_Speed'\n",
    "#         , 'Max_Impulse'\n",
    "#         , 'Mean_Impulse'\n",
    "#         , 'Max_Torque'\n",
    "#         , 'Mean_Torque'\n",
    "#         , 'Max_Int_Torque'\n",
    "#         , 'Mean_Int_Torque'\n",
    "      \n",
    "#     ]).sort(\"PlayKey\")\n",
    "\n",
    "\n",
    "#     return result\n",
    "\n",
    "# # Join the Qualitative with the Quantitative to create Summary Table\n",
    "# def qual_quant_merger(quals, quant):\n",
    "#     from DataHandler import data_shrinker\n",
    "#     qual_quant = quals.join(quant, on=\"PlayKey\", how=\"left\")\n",
    "#     qual_quant = data_shrinker(qual_quant)\n",
    "\n",
    "#     return qual_quant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing the Injury Data \n",
    "Since the data is getting prohibitively large, I'm breaking it into chunks while maintaining PlayKeys for the data, since I cannot break up the data across these. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "lazy_df = pl.scan_csv(\"F:/Data/nfl-playing-surface-analytics/PlayerTrackData.csv\").drop(['event', 's'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lazy_df.head(100).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import os\n",
    "import math\n",
    "\n",
    "\n",
    "def calculate_angle_difference(angle1, angle2):\n",
    "    \"\"\"\n",
    "    Calculate the smallest angle difference between two angles \n",
    "    using trigonometric functions, accounting for edge cases.\n",
    "    \"\"\"\n",
    "    import numpy as np # type: ignore\n",
    "\n",
    "    sin_diff = np.sin(np.radians(angle2 - angle1))\n",
    "    cos_diff = np.cos(np.radians(angle2 - angle1))\n",
    "    return np.degrees(np.arctan2(sin_diff, cos_diff))\n",
    "\n",
    "def angle_corrector(df):\n",
    "    \"\"\"\n",
    "    Make corrections to angles to reduce fringe errors at 360\n",
    "    \"\"\"\n",
    "    import polars as pl # type: ignore\n",
    "\n",
    "    df = df.with_columns([\n",
    "        ((pl.col(\"dir\") + 180) % 360 - 180).alias(\"dir\")\n",
    "        , ((pl.col(\"o\") + 180) % 360 - 180).alias(\"o\")\n",
    "    ]).with_columns(\n",
    "        (calculate_angle_difference(pl.col(\"dir\"), pl.col(\"o\"))).abs().round(2).alias(\"Angle_Diff\")\n",
    "        )\n",
    "    \n",
    "    return df\n",
    "\n",
    "def velocity_calculator(df):\n",
    "    \"\"\"\n",
    "    Using the (X,Y) and time columns, perform calculations based on the \n",
    "    difference between two rows to find displacement, speed, direction \n",
    "    of motion, velocity in x and y components, and the angular velocities \n",
    "    of the direction of motion and orientations \n",
    "    \"\"\"\n",
    "    import numpy as np # type: ignore\n",
    "    import polars as pl # type: ignore\n",
    "\n",
    "    \n",
    "    return df.with_columns([\n",
    "        # Convert 'o' and 'dir' to radians\n",
    "        (pl.col(\"o\") * np.pi / 180).alias(\"o_rad\"),\n",
    "        (pl.col(\"dir\") * np.pi / 180).alias(\"dir_rad\")\n",
    "    ]).with_columns([\n",
    "        # Pre-calculate shifted values\n",
    "        pl.col(\"x\").shift(1).over(\"PlayKey\").alias(\"prev_x\")\n",
    "        , pl.col(\"y\").shift(1).over(\"PlayKey\").alias(\"prev_y\")\n",
    "        # , pl.col(\"time\").shift(1).over(\"PlayKey\").alias(\"prev_time\")\n",
    "        , pl.col(\"dir_rad\").shift(1).over(\"PlayKey\").alias(\"prev_dir\")\n",
    "        , pl.col(\"o_rad\").shift(1).over(\"PlayKey\").alias(\"prev_o\")\n",
    "    ]).with_columns([\n",
    "        # Calculate the component displacements \n",
    "          (pl.col(\"x\") - pl.col(\"prev_x\")).alias(\"dx\")\n",
    "        , (pl.col(\"y\") - pl.col(\"prev_y\")).alias(\"dy\")\n",
    "    ]).with_columns([\n",
    "        # Calculate displacement\n",
    "        ((pl.col(\"dx\")**2 + pl.col(\"dy\")**2)**0.5).alias(\"Displacement\")\n",
    "    ]).with_columns([\n",
    "        # Calculate speed\n",
    "        (pl.col(\"Displacement\") / 0.1).alias(\"Speed\")\n",
    "        # Calculate direction\n",
    "        , (np.degrees(np.arctan2(pl.col(\"dx\"), pl.col(\"dy\")))).alias(\"Direction\")\n",
    "        # Calculate velocity components\n",
    "        , (pl.col(\"dx\") / 0.1).alias(\"vx\")\n",
    "        , (pl.col(\"dy\") / 0.1).alias(\"vy\")\n",
    "        # Calculate angular velocities\n",
    "        , ((pl.col(\"dir_rad\") - pl.col(\"prev_dir\")) / 0.1).alias(\"omega_dir\")\n",
    "        , ((pl.col(\"o_rad\") - pl.col(\"prev_o\")) / 0.1).alias(\"omega_o\")\n",
    "    ]).with_columns([\n",
    "        ((pl.col(\"omega_dir\") - pl.col(\"omega_o\")).abs()).alias(\"omega_diff\")\n",
    "    ]).drop([\n",
    "        \"prev_x\", \"prev_y\", \"prev_dir\", \"prev_o\", \"dx\", \"dy\", \"o_rad\", \"dir_rad\"\n",
    "    ])\n",
    "\n",
    "def body_builder(df):\n",
    "    import polars as pl # type: ignore\n",
    "    from DataHandler import data_loader\n",
    "\n",
    "    body_data = pl.LazyFrame({\n",
    "        \"position\": [\"QB\", \"RB\", \"FB\", \"WR\", \"TE\", \"T\", \"G\", \"C\", \"DE\", \"DT\", \"NT\", \"LB\", \"OLB\", \"MLB\", \"CB\", \"S\", \"K\", \"P\", \"SS\", \"ILB\", \"FS\", \"LS\", \"DB\"]\n",
    "        # , \"Position_Name\": [\"Quarterback\", \"Running Back\", \"Fullback\", \"Wide Receiver\", \"Tight End\", \"Tackle\", \"Guard\", \"Center\", \"Defensive End\", \"Defensive Tackle\", \"Nose Tackle\", \"Linebacker\", \"Outside Linebacker\", \"Middle Linebacker\", \"Cornerback\", \"Safety\", \"Kicker\", \"Punter\", \"Strong Safety\", \"Inside Linebacker\", \"Free Safety\", \"Long Snapper\", \"Defensive Back\"]\n",
    "        , \"Height_m\": [1.91, 1.79, 1.85, 1.88, 1.96, 1.97, 1.90, 1.87, 1.97, 1.92, 1.88, 1.90, 1.90, 1.87, 1.82, 1.84, 1.83, 1.88, 1.84, 1.90, 1.84, 1.88, 1.82]\n",
    "        , \"Weight_kg\": [102.1, 95.3, 111.1, 90.7, 114.6, 140.6, 141.8, 136.1, 120.2, 141.8, 152.0, 110.0, 108.9, 113.4, 87.4, 95.9, 92.08, 97.52, 95.9, 110.0, 95.9, 108.86, 87.4]\n",
    "        , \"Chest_rad_m\": [0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191]\n",
    "        })\n",
    "\n",
    "    position = data_loader(dataset='play_positions', database='nfl_surface')\n",
    "    position = position.lazy().join(\n",
    "                    body_data\n",
    "                    , on='position'\n",
    "                    , how='left'\n",
    "                    )\n",
    "\n",
    "    del body_data\n",
    "\n",
    "    df = df.join(\n",
    "                position\n",
    "                , left_on='PlayKey'\n",
    "                , right_on='playkey'\n",
    "                , how='left'\n",
    "            )  \n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def impulse_calculator(df):\n",
    "    \"\"\"\n",
    "    Using the (X,Y) and time columns, perform calculations based on the velocities and changes \n",
    "    in velocites along with player mass to get the momentum and impulse, a measure that can \n",
    "    be assessed along with medical data related to concussions and injuries\n",
    "    \"\"\"\n",
    "    import numpy as np # type: ignore\n",
    "    import polars as pl # type: ignore\n",
    "    \n",
    "    return df.with_columns([\n",
    "        # Calculate the linear momentum for each instant\n",
    "        (pl.col('vx') * pl.col('Weight_kg')).alias('px')\n",
    "        , (pl.col('vy') * pl.col('Weight_kg')).alias('py')\n",
    "\n",
    "        # Calculate the moment of inertia of a rotating upright body (1/12 mr^2)\n",
    "        , (1/12 * pl.col('Weight_kg') * (pl.col('Chest_rad_m')**2)).alias('moment')\n",
    "        \n",
    "        # Calculate the moment of inertia of the upper body turning upright with respect to waist (70% mass)\n",
    "        , (1/12 * (pl.col('Weight_kg')*0.7) * (pl.col('Chest_rad_m')**2)).alias('moment_upper')\n",
    "    \n",
    "    ]).with_columns([\n",
    "          # Calculate the magnitude of linear momentum\n",
    "        ((pl.col(\"px\")**2 + pl.col(\"py\")**2)**0.5).alias(\"p_magnitude\")\n",
    "        \n",
    "        # Calculate the angular momentum for the direction\n",
    "        , (pl.col('omega_dir')*pl.col('moment')).alias('L_dir')\n",
    "\n",
    "        # Calculate the angular momentum of the upper body with respect to lower\n",
    "        , (pl.col('omega_diff')*pl.col('moment_upper')).alias('L_diff')\n",
    "\n",
    "\n",
    "    ]).with_columns([\n",
    "        # Pre-calculate shifted values for linear and angular momenta\n",
    "        pl.col(\"px\").shift(1).over(\"PlayKey\").alias(\"prev_px\")\n",
    "        , pl.col(\"py\").shift(1).over(\"PlayKey\").alias(\"prev_py\")\n",
    "        , pl.col(\"L_dir\").shift(1).over(\"PlayKey\").alias(\"prev_L_dir\")\n",
    "        , pl.col(\"L_diff\").shift(1).over(\"PlayKey\").alias(\"prev_L_diff\")\n",
    "        \n",
    "    ]).with_columns([\n",
    "        # Calculate impulse, J, which is the change in linear momentum \n",
    "        ((pl.col(\"px\") - pl.col(\"prev_px\"))).alias(\"Jx\")\n",
    "        , ((pl.col(\"py\") - pl.col(\"prev_py\"))).alias(\"Jy\")\n",
    "        \n",
    "    ]).with_columns([\n",
    "          # Calculate the magnitude of linear momentum\n",
    "        ((pl.col(\"Jx\")**2 + pl.col(\"Jy\")**2)**0.5).alias(\"J_magnitude\")\n",
    "\n",
    "        # Calculate torque as the change in angular momentum L over the change in time\n",
    "        , (((pl.col(\"L_dir\") - pl.col(\"prev_L_dir\"))) / 0.1).alias(\"torque\")\n",
    "        , (((pl.col(\"L_diff\") - pl.col(\"prev_L_diff\"))) / 0.1).alias(\"torque_internal\")\n",
    "\n",
    "    ]).drop([\n",
    "        \"prev_L_dir\", \"prev_px\", \"prev_py\", \"prev_L_diff\"\n",
    "    ])\n",
    "\n",
    "def path_calculator(df):\n",
    "    \"\"\"\n",
    "    Collects dispalcement and distance, means and maxima for the for each of the parameters collected\n",
    "    and outputs to a quantitative summary table that can be joined to the qualitative table for machine learning.  \n",
    "    \"\"\"\n",
    "    import polars as pl # type: ignore\n",
    "\n",
    "    result = df.select([\n",
    "        \"PlayKey\"\n",
    "        , pl.col(\"Displacement\").sum().over(\"PlayKey\").alias(\"Distance\")\n",
    "        , pl.col(\"x\").first().over(\"PlayKey\").alias(\"start_x\")\n",
    "        , pl.col(\"y\").first().over(\"PlayKey\").alias(\"start_y\")\n",
    "        , pl.col(\"x\").last().over(\"PlayKey\").alias(\"end_x\")\n",
    "        , pl.col(\"y\").last().over(\"PlayKey\").alias(\"end_y\")\n",
    "        , pl.col(\"Angle_Diff\").max().over(\"PlayKey\").alias(\"Max_Angle_Diff\")\n",
    "        , pl.col(\"Angle_Diff\").mean().over(\"PlayKey\").alias(\"Mean_Angle_Diff\")\n",
    "        , pl.col(\"Speed\").max().over(\"PlayKey\").alias(\"Max_Speed\")\n",
    "        , pl.col(\"Speed\").mean().over(\"PlayKey\").alias(\"Mean_Speed\")\n",
    "        , pl.col(\"J_magnitude\").max().over(\"PlayKey\").alias(\"Max_Impulse\")\n",
    "        , pl.col(\"J_magnitude\").mean().over(\"PlayKey\").alias(\"Mean_Impulse\")\n",
    "        , pl.col(\"torque\").max().over(\"PlayKey\").alias(\"Max_Torque\")\n",
    "        , pl.col(\"torque\").mean().over(\"PlayKey\").alias(\"Mean_Torque\")\n",
    "        , pl.col(\"torque_internal\").max().over(\"PlayKey\").alias(\"Max_Int_Torque\")\n",
    "        , pl.col(\"torque_internal\").mean().over(\"PlayKey\").alias(\"Mean_Int_Torque\")\n",
    "\n",
    "        ]).unique(subset=[\"PlayKey\"])\n",
    "\n",
    "\n",
    "    # Calculate the displacement\n",
    "    result = result.with_columns([\n",
    "        (((pl.col(\"end_x\") - pl.col(\"start_x\"))**2 + \n",
    "          (pl.col(\"end_y\") - pl.col(\"start_y\"))**2)**0.5)\n",
    "        .alias(\"Displacement\")\n",
    "        ]).with_columns([\n",
    "            (pl.col(\"Distance\") - pl.col(\"Displacement\")).alias(\"Path_Diff\")\n",
    "        ])\n",
    "\n",
    "     \n",
    "    # Select only the required columns\n",
    "    result = result.select([\n",
    "        'PlayKey'\n",
    "        , 'Distance'\n",
    "        , 'Displacement'\n",
    "        , 'Path_Diff'\n",
    "        , 'Max_Angle_Diff'\n",
    "        , 'Mean_Angle_Diff'\n",
    "        , 'Max_Speed'\n",
    "        , 'Mean_Speed'\n",
    "        , 'Max_Impulse'\n",
    "        , 'Mean_Impulse'\n",
    "        , 'Max_Torque'\n",
    "        , 'Mean_Torque'\n",
    "        , 'Max_Int_Torque'\n",
    "        , 'Mean_Int_Torque'\n",
    "      \n",
    "    ]).sort(\"PlayKey\")\n",
    "\n",
    "\n",
    "    return result\n",
    "\n",
    "#####################################################################################\n",
    "\n",
    "def process_and_save_playkey_group(lazy_df, playkeys, output_dir, group_number):\n",
    "    # Filter the lazy DataFrame for the specific PlayKeys\n",
    "    group_df = lazy_df.filter(pl.col(\"PlayKey\").is_in(playkeys))\n",
    "    \n",
    "    # Processing\n",
    "    group_df = angle_corrector(group_df)\n",
    "    group_df = velocity_calculator(group_df)\n",
    "    group_df = body_builder(group_df)\n",
    "    group_df = impulse_calculator(group_df)\n",
    "\n",
    "\n",
    "    # Collect the data\n",
    "    processed_df = group_df.collect()\n",
    "    processed_df = data_shrinker(processed_df)\n",
    " \n",
    "\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "\n",
    "    # Save the DataFrame for this group as a CSV file\n",
    "    output_file = os.path.join(output_dir, f\"group_{group_number}.csv\")\n",
    "    processed_df.write_csv(output_file)\n",
    "    print(f\"Saved data for PlayKey group: {group_number}\")\n",
    "\n",
    "\n",
    "def process_and_save_summary(lazy_df, playkeys, output_dir, group_number):\n",
    "    # Filter the lazy DataFrame for the specific PlayKeys\n",
    "    group_df = lazy_df.filter(pl.col(\"PlayKey\").is_in(playkeys))\n",
    "    \n",
    "    # Processing\n",
    "    group_df = angle_corrector(group_df)\n",
    "    group_df = velocity_calculator(group_df)\n",
    "    group_df = body_builder(group_df)\n",
    "    group_df = impulse_calculator(group_df)\n",
    "\n",
    "    # Calculate the path summary quant data\n",
    "    summary_df = path_calculator(group_df)\n",
    "\n",
    "    # Collect the data\n",
    "    summary_df = summary_df.collect()\n",
    "    summary_df = data_shrinker(summary_df)\n",
    "\n",
    "\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "\n",
    "    # Save the summary data for this group in a separate csv file\n",
    "    summary_file = os.path.join(output_dir, f\"summary_group_{group_number}.csv\")\n",
    "    summary_df.write_csv(summary_file)\n",
    "    \n",
    "    print(f\"Saved data for PlayKey group: {group_number}\")\n",
    "\n",
    "\n",
    "def process_csv(analysis_type, input_file, output_dir, group_size=10000):\n",
    "    # Scan the CSV file\n",
    "    lazy_df = pl.scan_csv(input_file, truncate_ragged_lines=True, infer_schema_length=10000, ignore_errors=True).drop(['event', 's'])\n",
    "    \n",
    "    # Get unique PlayKey values\n",
    "    unique_playkeys = lazy_df.select(pl.col(\"PlayKey\").unique()).collect()[\"PlayKey\"].to_list()\n",
    "    \n",
    "    # Calculate the number of groups\n",
    "    num_groups = math.ceil(len(unique_playkeys) / group_size)\n",
    "    \n",
    "\n",
    "    if analysis_type == \"tracking\":\n",
    "        # Process each group of PlayKeys\n",
    "        for i in range(num_groups):\n",
    "            start_idx = i * group_size\n",
    "            end_idx = min((i + 1) * group_size, len(unique_playkeys))\n",
    "            playkey_group = unique_playkeys[start_idx:end_idx]\n",
    "            process_and_save_playkey_group(lazy_df, playkey_group, output_dir, i + 1)\n",
    "\n",
    "    elif analysis_type == \"summary\":\n",
    "        # Process each group of Summary\n",
    "        for i in range(num_groups):\n",
    "            start_idx = i * group_size\n",
    "            end_idx = min((i + 1) * group_size, len(unique_playkeys))\n",
    "            playkey_group = unique_playkeys[start_idx:end_idx]\n",
    "            process_and_save_summary(lazy_df, playkey_group, output_dir, i + 1)\n",
    "\n",
    "    print(\"Processing complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    input_file = \"F:/Data/Processing_data/tracking.csv\"\n",
    "    output_dir = \"F:/Data/Processing_data/output\"\n",
    "    \n",
    "    process_csv('tracking', input_file, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    input_file = \"F:/Data/Processing_data/tracking.csv\"\n",
    "    output_dir = \"F:/Data/Processing_data/output\"\n",
    "    \n",
    "    process_csv('summary', input_file, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing the Concussion Data \n",
    "Similar to the processing of the Injury data, but with some variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import os\n",
    "from TransformFunctions import *\n",
    "\n",
    "def process_ngs_file(input_file, output_dir):\n",
    "    # Extract the base name of the input file\n",
    "    base_name = os.path.splitext(os.path.basename(input_file))[0]\n",
    "    \n",
    "    # Read the CSV file\n",
    "    lazy_df = pl.scan_csv(input_file, truncate_ragged_lines=True, ignore_errors=True).drop(['Event', 'dis', 'Season_Year'])\n",
    "    \n",
    "    # Apply the processing steps\n",
    "    lazy_df = column_corrector(lazy_df)\n",
    "    lazy_df = angle_corrector(lazy_df)\n",
    "    lazy_df = body_builder(lazy_df, 'ngs_data')\n",
    "    lazy_df = velocity_calculator(lazy_df)\n",
    "    lazy_df = impulse_calculator(lazy_df)\n",
    "    \n",
    "    # Calculate the path summary\n",
    "    lazy_summary = path_calculator(lazy_df)\n",
    "    \n",
    "    # Create output filenames\n",
    "    processed_output = os.path.join(output_dir, f\"{base_name}_processed.csv\")\n",
    "    summary_output = os.path.join(output_dir, f\"{base_name}_summary.csv\")\n",
    "    \n",
    "    # Save the processed data and summary\n",
    "    lazy_df.collect().write_csv(processed_output)\n",
    "    lazy_summary.collect().write_csv(summary_output)\n",
    "    \n",
    "    print(f\"Processed and saved data for file: {input_file}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Main execution\n",
    "# if __name__ == \"__main__\":\n",
    "#     input_dir = \"F:/Data/Processing_data/NGS\"\n",
    "#     output_dir = \"F:/Data/Processing_data/output\"\n",
    "    \n",
    "#     # Ensure output directory exists\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "#     # Process all NGS-*.csv files in the input directory\n",
    "#     for filename in os.listdir(input_dir):\n",
    "#         if filename.startswith(\"NGS\") and filename.endswith(\".csv\"):\n",
    "#             input_file = os.path.join(input_dir, filename)\n",
    "#             process_ngs_file(input_file, output_dir)\n",
    "    \n",
    "#     print(\"All files processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved data for file: F:/Data/Processing_data/NGS/NGS-2017-reg-wk7-12.csv\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    output_dir = \"F:/Data/Processing_data/output\"\n",
    "    input_file = \"F:/Data/Processing_data/NGS/NGS-2017-reg-wk7-12.csv\"\n",
    "    \n",
    "    process_ngs_file(input_file, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved data for file: F:/Data/Processing_data/NGS/NGS-2017-reg-wk13-17.csv\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    output_dir = \"F:/Data/Processing_data/output\"\n",
    "    input_file = \"F:/Data/Processing_data/NGS/NGS-2017-reg-wk13-17.csv\"\n",
    "    \n",
    "    process_ngs_file(input_file, output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
