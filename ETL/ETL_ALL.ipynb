{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETL for both Concussion and Surface Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import sqlalchemy as db\n",
    "from sqlalchemy.orm import Session\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "\n",
    "from CleaningFunctions import *\n",
    "from TransformFunctions import *\n",
    "from DataHandler import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the set of functions from the concussion sets. Most of these overlap with the surface injury sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Play and Injury Data has been cleaned and uploaded as qualitative\n"
     ]
    }
   ],
   "source": [
    "clean_injuries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concussion Data has been cleaned and uploaded as clean_data\n"
     ]
    }
   ],
   "source": [
    "clean_concussions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 0.63 MB\n",
      "Memory usage after optimization is: 0.40 MB\n",
      "Decreased by 36.5%\n",
      "Memory usage of dataframe is 36.66 MB\n",
      "Memory usage after optimization is: 23.92 MB\n",
      "Decreased by 34.7%\n",
      "Writing all quantitative and qualitative summary data to the database as summary_data. Wait.\n",
      "Data has been uploaded to the database. Probably.\n"
     ]
    }
   ],
   "source": [
    "transform_injury_data('summary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 0.42 MB\n",
      "Memory usage after optimization is: 0.37 MB\n",
      "Decreased by 11.6%\n",
      "Memory usage of dataframe is 25.36 MB\n",
      "Memory usage after optimization is: 20.68 MB\n",
      "Decreased by 18.4%\n",
      "Writing all quantitative and qualitative summary data to the database as summary_data. Wait.\n",
      "Data has been uploaded to the database. Probably.\n"
     ]
    }
   ],
   "source": [
    "transform_concussion_data('summary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 0.63 MB\n",
      "Memory usage after optimization is: 0.40 MB\n",
      "Decreased by 36.5%\n",
      "Writing the transformed table with physical parameters to the database as quantitative\n",
      "Data has been uploaded to the database. Go celebrate!\n"
     ]
    }
   ],
   "source": [
    "transform_injury_data('tracking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 0.42 MB\n",
      "Memory usage after optimization is: 0.37 MB\n",
      "Decreased by 11.6%\n",
      "Writing the transformed table with physical parameters to the database as quantitative\n",
      "Data has been uploaded to the database. Good for you!\n"
     ]
    }
   ],
   "source": [
    "transform_concussion_data('tracking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cleaning Functions\n",
    "# # This will set up the code for all cleaning functions. Currently, this cleans only the injury data for surface injuries. \n",
    "# # TO RUN: \n",
    "#     # from CleaningFunctions import *\n",
    "#     # clean_injuries()\n",
    "\n",
    "\n",
    "# ##### Primary Cleaning Functions #####\n",
    "# def clean_injuries():\n",
    "#     from DataHandler import data_writer\n",
    "#     database = \"nfl_surface\"\n",
    "#     quals = table_joiner() \n",
    "#     quals = column_capitalizer(quals, df_name='quals')\n",
    "#     quals = stadium_cleaner(quals, df_name='quals')\n",
    "#     quals = weather_cleaner(quals)\n",
    "#     quals = injury_cleaner(quals)\n",
    "#     data_writer(quals, database, \"qualitative\")\n",
    "#     print(\"Play and Injury Data has been cleaned and uploaded as qualitative\")\n",
    "#     del quals\n",
    "\n",
    "\n",
    "# def clean_concussions():\n",
    "#     from DataHandler import data_loader, data_writer\n",
    "#     import polars as pl\n",
    "#     database = 'nfl_concussion'\n",
    "\n",
    "#     df = data_loader(database='nfl_concussion', dataset='concussion')\n",
    "#     df = column_capitalizer(df, 'concussion')\n",
    "#     df = stadium_cleaner(df, 'concussion')\n",
    "#     df = weather_cleaner(df)\n",
    "#     df = turf_cleaner(df)\n",
    "#     df = df.filter(pl.col(\"Game_Date\").is_not_null())\n",
    "#     df = score_splitter(df)\n",
    "#     data_writer(df, database, \"clean_data\")\n",
    "#     print(\"Concussion Data has been cleaned and uploaded as clean_data\")\n",
    "#     # return df\n",
    "\n",
    "\n",
    "\n",
    "# ######################################################################################\n",
    "# # Extracts and joins the necessary columns from the Injuries and Plays tables\n",
    "# def table_joiner():\n",
    "#     import polars as pl\n",
    "#     from DataHandler import data_loader\n",
    "\n",
    "#     plays = data_loader('plays', 'nfl_surface')\n",
    "#     injuries = data_loader('injuries', 'nfl_surface')\n",
    "\n",
    "#     quals = (\n",
    "#         plays.join(injuries, on=\"playkey\", how='left')\n",
    "#         .select([\n",
    "#             pl.col(\"playkey\")\n",
    "#             , pl.col(\"rosterposition\")\n",
    "#             , pl.col(\"stadiumtype\")\n",
    "#             , pl.col(\"fieldtype\")\n",
    "#             , pl.col(\"temperature\")\n",
    "#             , pl.col(\"weather\")\n",
    "#             , pl.col(\"playtype\")\n",
    "#             , pl.col(\"bodypart\")\n",
    "#             , pl.col(\"dm_m1\")\n",
    "#             , pl.col(\"dm_m7\")\n",
    "#             , pl.col(\"dm_m28\")\n",
    "#             , pl.col(\"dm_m42\")\n",
    "\n",
    "#         ])\n",
    "#     )\n",
    "#     return quals\n",
    "\n",
    "\n",
    "# # Changes the all lower-case to Capitalized PascalCase column headers \n",
    "# def column_capitalizer(df, df_name):          \n",
    "#     if df_name == 'quals':\n",
    "#         columns = {\n",
    "#         'playkey': \"PlayKey\"\n",
    "#         , 'position': 'Position'\n",
    "#         , 'stadiumtype': 'Stadium_Type'\n",
    "#         , 'fieldtype': 'Field_Type'\n",
    "#         , 'temperature': 'Temperature'\n",
    "#         , 'weather': 'Weather'\n",
    "#         , 'playtype': 'Play_Type'\n",
    "#         , 'bodypart': 'Body_Part'\n",
    "#         , 'dm_m1': 'DM_1'\n",
    "#         , 'dm_m7': 'DM_7'\n",
    "#         , 'dm_m28': 'DM_28'\n",
    "#         , 'dm_m42': 'DM_42'\n",
    "#         }\n",
    "\n",
    "#     elif df_name == 'concussion':\n",
    "#         columns = {\n",
    "#         'playkey': 'Playkey'\n",
    "#         , 'position': 'Position'\n",
    "#         , 'role': 'Role'\n",
    "#         , 'game_date': 'Game_Date'\n",
    "#         , 'yardline': 'Yardline'\n",
    "#         , 'quarter': 'Quarter'\n",
    "#         , 'play_type': 'Play_Type'\n",
    "#         , 'poss_team': 'Poss_Team'\n",
    "#         , 'score_home_visiting': 'Score_Home_Visiting'\n",
    "#         , 'game_site': 'Game_Site'\n",
    "#         , 'start_time': 'Start_Time'\n",
    "#         , 'hometeamcode': 'Home_Team_Code'\n",
    "#         , 'visitteamcode': 'Visit_Team_Code'\n",
    "#         , 'stadiumtype': 'Stadium_Type'\n",
    "#         , 'turf': 'Field_Type'\n",
    "#         , 'gameweather': 'Weather'\n",
    "#         , 'temperature': 'Temperature'\n",
    "#         , 'player_activity_derived': 'Player_Activity_Derived'\n",
    "#         , 'primary_impact_type': 'Primary_Impact_Type'\n",
    "#         , 'primary_partner_activity_derived': 'Primary_Partner_Activity_Derived'\n",
    "#         , 'primary_partner_gsisid': 'Primary_Partner_Gsisid'\n",
    "#         }\n",
    "\n",
    "\n",
    "#     df = df.rename(columns)\n",
    "#     return df\n",
    "\n",
    "# # This changes stadiums to either Indoor or Outdoor per game records - some of the dome stadiums have a roof that can open, if open the game is considered outdoor.\n",
    "# def stadium_cleaner(df, df_name):\n",
    "#     import polars as pl \n",
    "\n",
    "#     if df_name == 'quals':       \n",
    "#         stadium_dict = {\n",
    "#             'Outdoor': 'Outdoor'\n",
    "#             , 'Indoors': 'Indoor'\n",
    "#             , 'Oudoor': 'Outdoor'\n",
    "#             , 'Outdoors': 'Outdoor'\n",
    "#             , 'Open': 'Outdoor'\n",
    "#             , 'Closed Dome': 'Indoor'\n",
    "#             , 'Domed, closed': 'Indoor'\n",
    "#             , 'Dome': 'Indoor'\n",
    "#             , 'Indoor': 'Indoor'\n",
    "#             , 'Domed': 'Indoor'\n",
    "#             , 'Retr. Roof-Closed': 'Indoor'\n",
    "#             , 'Outdoor Retr Roof-Open': 'Outdoor'\n",
    "#             , 'Retractable Roof': 'Indoor'\n",
    "#             , 'Ourdoor': 'Outdoor'\n",
    "#             , 'Indoor, Roof Closed': 'Indoor'\n",
    "#             , 'Retr. Roof - Closed': 'Indoor'\n",
    "#             , 'Bowl': 'Outdoor'\n",
    "#             , 'Outddors': 'Outdoor'\n",
    "#             , 'Retr. Roof-Open': 'Outdoor'\n",
    "#             , 'Dome, closed': 'Indoor'\n",
    "#             , 'Indoor, Open Roof': 'Outdoor'\n",
    "#             , 'Domed, Open': 'Outdoor'\n",
    "#             , 'Domed, open': 'Outdoor'\n",
    "#             , 'Heinz Field': 'Outdoor'\n",
    "#             , 'Cloudy': 'Outdoor'\n",
    "#             , 'Retr. Roof - Open': 'Outdoor'\n",
    "#             , 'Retr. Roof Closed': 'Indoor'\n",
    "#             , 'Outdor': 'Outdoor'\n",
    "#             , 'Outside': 'Outdoor'\n",
    "#         }\n",
    "\n",
    "\n",
    "#         df = df.with_columns(pl.col(\"Stadium_Type\").fill_null(\"Outdoor\")) # Since most stadiums are outdoor and the percentage of games played indoor is already met by the known indoor games those seasons, all unknown games were set to outdoor\n",
    "\n",
    "\n",
    "#     elif df_name == 'concussion':\n",
    "#         stadium_dict = {\n",
    "#             'Outdoor': 'Outdoor'\n",
    "#             , 'outdoor': 'Outdoor'\n",
    "#             , 'Indoors': 'Indoor'\n",
    "#             , 'Indoors (Domed)': 'Indoor'\n",
    "#             , 'Oudoor': 'Outdoor'\n",
    "#             , 'Outdoors': 'Outdoor'\n",
    "#             , 'Outdoors ': 'Outdoor'\n",
    "#             , 'Open': 'Outdoor'\n",
    "#             , 'Closed Dome': 'Indoor'\n",
    "#             , 'Domed, closed': 'Indoor'\n",
    "#             , 'Dome': 'Indoor'\n",
    "#             , 'Indoor': 'Indoor'\n",
    "#             , 'Domed': 'Indoor'\n",
    "#             , 'Retr. Roof-Closed': 'Indoor'\n",
    "#             , 'Outdoor Retr Roof-Open': 'Outdoor'\n",
    "#             , 'Retractable Roof': 'Indoor'\n",
    "#             , 'Ourdoor': 'Outdoor'\n",
    "#             , 'Indoor, Roof Closed': 'Indoor'\n",
    "#             , 'Retr. Roof - Closed': 'Indoor'\n",
    "#             , 'Bowl': 'Outdoor'\n",
    "#             , 'Outddors': 'Outdoor'\n",
    "#             , 'Retr. Roof-Open': 'Outdoor'\n",
    "#             , 'Dome, closed': 'Indoor'\n",
    "#             , 'Indoor, Open Roof': 'Outdoor'\n",
    "#             , 'Domed, Open': 'Outdoor'\n",
    "#             , 'Domed, open': 'Outdoor'\n",
    "#             , 'Heinz Field': 'Outdoor'\n",
    "#             , 'Cloudy': 'Outdoor'\n",
    "#             , 'Retr. Roof - Open': 'Outdoor'\n",
    "#             , 'Retr. Roof Closed': 'Indoor'\n",
    "#             , 'Outdor': 'Outdoor'\n",
    "#             , 'Outside': 'Outdoor'\n",
    "#             , 'Indoor, non-retractable roof': 'Indoor'\n",
    "#             , 'Retr. roof - closed': 'Indoor'\n",
    "#             , 'Indoor, fixed roof ': 'Indoor'\n",
    "#             , 'Indoor, Non-Retractable Dome': 'Indoor'\n",
    "#             , 'Indoor, Fixed Roof': 'Indoor'\n",
    "#             , 'Indoor, fixed roof': 'Indoor'\n",
    "#             , None: 'Outdoor'  # It was verified that all fields with null values are indeed outdoor\n",
    "#         }\n",
    "\n",
    "\n",
    "#     df = df.with_columns(pl.col(\"Stadium_Type\").replace(stadium_dict)) # This uses the dict to assign naming conventions\n",
    "\n",
    "#     return df\n",
    "\n",
    "# # Cleans up the weather data from having a lot of different but similar to a few categories\n",
    "# def weather_cleaner(df):\n",
    "#      import polars as pl\n",
    "     \n",
    "#      weather_dict = {\n",
    "#             'Clear and warm': 'Clear'\n",
    "#             , 'Mostly Cloudy': 'Cloudy'\n",
    "#             , 'Sunny': 'Clear'\n",
    "#             , 'Clear': 'Clear'\n",
    "#             , 'Cloudy': 'Cloudy'\n",
    "#             , 'Cloudy, fog started developing in 2nd quarter': 'Hazy/Fog'\n",
    "#             , 'Rain': 'Rain'\n",
    "#             , 'Partly Cloudy': 'Cloudy'\n",
    "#             , 'Mostly cloudy': 'Cloudy'\n",
    "#             , 'Cloudy and cold': 'Cloudy'\n",
    "#             , 'Cloudy and Cool': 'Cloudy'\n",
    "#             , 'Rain Chance 40%': 'Rain'\n",
    "#             , 'Controlled Climate': 'Indoor'\n",
    "#             , 'Sunny and warm': 'Clear'\n",
    "#             , 'Partly cloudy': 'Cloudy'\n",
    "#             , 'Clear and Cool': 'Cloudy'\n",
    "#             , 'Clear and cold': 'Cloudy'\n",
    "#             , 'Sunny and cold': 'Clear'\n",
    "#             , 'Indoor': 'Indoor'\n",
    "#             , 'Partly Sunny': 'Clear'\n",
    "#             , 'N/A (Indoors)': 'Indoor'\n",
    "#             , 'Mostly Sunny': 'Clear'\n",
    "#             , 'Indoors': 'Indoor'\n",
    "#             , 'Clear Skies': 'Clear'\n",
    "#             , 'Partly sunny': 'Clear'\n",
    "#             , 'Showers': 'Rain'\n",
    "#             , 'N/A Indoor': 'Indoor'\n",
    "#             , 'Sunny and clear': 'Clear'\n",
    "#             , 'Snow': 'Snow'\n",
    "#             , 'Scattered Showers': 'Rain'\n",
    "#             , 'Party Cloudy': 'Cloudy'\n",
    "#             , 'Clear skies': 'Clear'\n",
    "#             , 'Rain likely, temps in low 40s.': 'Rain'\n",
    "#             , 'Hazy': 'Hazy/Fog'\n",
    "#             , 'Partly Clouidy': 'Cloudy'\n",
    "#             , 'Sunny Skies': 'Clear'\n",
    "#             , 'Overcast': 'Cloudy'\n",
    "#             , 'Cloudy, 50% change of rain': 'Cloudy'\n",
    "#             , 'Fair': 'Clear'\n",
    "#             , 'Light Rain': 'Rain'\n",
    "#             , 'Partly clear': 'Clear'\n",
    "#             , 'Mostly Coudy': 'Cloudy'\n",
    "#             , '10% Chance of Rain': 'Cloudy'\n",
    "#             , 'Cloudy, chance of rain': 'Cloudy'\n",
    "#             , 'Heat Index 95': 'Clear'\n",
    "#             , 'Sunny, highs to upper 80s': 'Clear'\n",
    "#             , 'Sun & clouds': 'Cloudy'\n",
    "#             , 'Heavy lake effect snow': 'Snow'\n",
    "#             , 'Mostly sunny': 'Clear'\n",
    "#             , 'Cloudy, Rain': 'Rain'\n",
    "#             , 'Sunny, Windy': 'Windy'\n",
    "#             , 'Mostly Sunny Skies': 'Clear'\n",
    "#             , 'Rainy': 'Rain'\n",
    "#             , '30% Chance of Rain': 'Rain'\n",
    "#             , 'Cloudy, light snow accumulating 1-3\"': 'Snow'\n",
    "#             , 'cloudy': 'Cloudy'\n",
    "#             , 'Clear and Sunny': 'Clear'\n",
    "#             , 'Coudy': 'Cloudy'\n",
    "#             , 'Clear and sunny': 'Clear'\n",
    "#             , 'Clear to Partly Cloudy': 'Clear'\n",
    "#             , 'Cloudy with periods of rain, thunder possible. Winds shifting to WNW, 10-20 mph.': 'Windy'\n",
    "#             , 'Rain shower': 'Rain'\n",
    "#             , 'Cold': 'Clear'\n",
    "#             , 'Partly cloudy, lows to upper 50s.': 'Cloudy'\n",
    "#             , 'Scattered thunderstorms': 'Rain'\n",
    "#             , 'CLEAR': 'Clear'\n",
    "#             , 'Partly CLoudy': 'Cloudy'\n",
    "#             , 'Chance of Showers': 'Rain'\n",
    "#             , 'Snow showers': 'Snow'\n",
    "#             , 'Clear and Cold': 'Clear'\n",
    "#             , 'Cloudy with rain': 'Rain'\n",
    "#             , 'Sunny intervals': 'Clear'\n",
    "#             , 'Clear and cool': 'Clear'\n",
    "#             , 'Cloudy, Humid, Chance of Rain': 'Rain'\n",
    "#             , 'Cloudy and Cold': 'Cloudy'\n",
    "#             , 'Cloudy with patches of fog': 'Hazy/Fog'\n",
    "#             , 'Controlled': 'Indoor'\n",
    "#             , 'Sunny and Clear': 'Clear'\n",
    "#             , 'Cloudy with Possible Stray Showers/Thundershowers': 'Rain'\n",
    "#             , 'Suny': 'Clear'\n",
    "#             , 'T-Storms': 'Rain'\n",
    "#             , 'Sunny and cool': 'Clear'\n",
    "#             , 'Cloudy, steady temps': 'Cloudy'\n",
    "#             , 'Hazy, hot and humid': 'Hazy/Fog'\n",
    "#             , 'Sunny Intervals': 'Clear'\n",
    "#             , 'Partly Cloudy, Chance of Rain 80%': 'Rain'\n",
    "#             , 'Mostly Clear. Gusting ot 14.': 'Windy'\n",
    "#             , 'Mostly CLoudy': 'Cloudy'\n",
    "#             , 'Snow Showers, 3 to 5 inches expected.': 'Snow'\n",
    "#             }\n",
    "\n",
    "\n",
    "#      df = df.with_columns(pl.col(\"Weather\").replace(weather_dict)) # Standardizes the weather to a few main types\n",
    "\n",
    "#      df = df.with_columns(             # Null handling - all null weather conditions for indoor stadiums are filled \"indoor\"\n",
    "#                 pl.when(pl.col(\"Stadium_Type\") == \"Indoor\")\n",
    "#                 .then(pl.col(\"Weather\").fill_null(\"Indoor\"))\n",
    "#                 .otherwise(pl.col(\"Weather\"))\n",
    "#                 .alias(\"Weather\")\n",
    "#                 )\n",
    "     \n",
    "#      # For the non-indoor games with null values for weather, to maintain the percentage of games that were clear/cloudy, temperature was used as a divider, above and below 70 degrees\n",
    "#      df = df.with_columns(\n",
    "#                 pl.when(pl.col(\"Temperature\") > 70)\n",
    "#                 .then(pl.col(\"Weather\").fill_null(\"Clear\"))\n",
    "#                 .otherwise(pl.col(\"Weather\"))\n",
    "#                 .alias(\"Weather\")\n",
    "#                 )\n",
    "#      df = df.with_columns(pl.col(\"Weather\").fill_null(\"Cloudy\"))\n",
    "\n",
    "#      return df\n",
    "\n",
    "\n",
    "# # This fixes the issues with introduced nulls following the joins \n",
    "# def injury_cleaner(quals):\n",
    "#     import polars as pl\n",
    "#     quals = quals.filter(pl.col('Play_Type').is_not_null()) # 0.14% of rows did not have a play type, and ALL of these were non-injury plays, so they were removed\n",
    "\n",
    "#     quals = quals.with_columns(pl.col(\"Body_Part\").fill_null(\"No_Injury\")) # This fills all null from the join with No Injury\n",
    "\n",
    "#     quals = quals.with_columns(\n",
    "#     pl.col([\"DM_1\", \"DM_7\", \"DM_28\", \"DM_42\"]).fill_null(0)) # This fills the nulls from the Join with 0s, since there were no injuries.\n",
    "\n",
    "#     return quals\n",
    "\n",
    "\n",
    "# # This will standardize the types of Turf for the FieldType to either natural or synthetic\n",
    "\n",
    "# def turf_cleaner(df):\n",
    "#     ''' \n",
    "#     Changes the many different types of turf listed into either natural or synthetic\n",
    "#     '''\n",
    "#     import polars as pl\n",
    "\n",
    "#     turf_dict = {\n",
    "#         'Grass': 'Natural'\n",
    "#         , 'Field Turf': 'Synthetic'\n",
    "#         , 'Natural Grass': 'Natural'\n",
    "#         , 'grass': 'Natural'\n",
    "#         , 'Artificial': 'Synthetic'\n",
    "#         , 'FieldTurf': 'Synthetic'\n",
    "#         , 'DD GrassMaster': 'Synthetic'\n",
    "#         , 'A-Turf Titan': 'Synthetic'\n",
    "#         , 'UBU Sports Speed S5-M': 'Synthetic'\n",
    "#         , 'UBU Speed Series S5-M': 'Synthetic'\n",
    "#         , 'Artifical': 'Synthetic'\n",
    "#         , 'UBU Speed Series-S5-M': 'Synthetic'\n",
    "#         , 'FieldTurf 360': 'Synthetic'\n",
    "#         , 'Natural grass': 'Natural'\n",
    "#         , 'Field turf': 'Synthetic'\n",
    "#         , 'Natural': 'Natural'\n",
    "#         , 'Natrual Grass': 'Natural'\n",
    "#         , 'Synthetic': 'Synthetic'\n",
    "#         , 'Natural Grass ': 'Natural'\n",
    "#         , 'Naturall Grass': 'Natural'\n",
    "#         , 'FieldTurf360': 'Synthetic'\n",
    "#         , None: 'Natural'  # The only field with null values is Miami Gardens, which has Natural\n",
    "#         }\n",
    "\n",
    "    \n",
    "#     df = df.with_columns(pl.col(\"Field_Type\").replace(turf_dict))\n",
    "#     return df\n",
    "\n",
    "# def score_splitter(df):\n",
    "#     ''' \n",
    "#     Splits the string column from Score_Home_Visiting into two numeric columns for each of the scores. It also creates a column that calculates the difference. \n",
    "#     '''\n",
    "#     import polars as pl\n",
    "\n",
    "#     df = df.with_columns([\n",
    "#         pl.col(\"Score_Home_Visiting\").str.extract(r\"(\\d+)\\s*-\\s*(\\d+)\", 1).cast(pl.Int16).alias(\"Home_Score\")\n",
    "#         , pl.col(\"Score_Home_Visiting\").str.extract(r\"(\\d+)\\s*-\\s*(\\d+)\", 2).cast(pl.Int16).alias(\"Visiting_Score\") # Find difference between scores\n",
    "#         ])\n",
    "\n",
    "#     df = df.with_columns([\n",
    "#         (pl.col(\"Home_Score\") - pl.col(\"Visiting_Score\")).cast(pl.Int16).alias(\"Score_Difference\")\n",
    "#         ])\n",
    "    \n",
    "#     df = df.drop(\"Score_Home_Visiting\")\n",
    "    \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def transform_injury_data(output):\n",
    "#     from DataHandler import data_loader, data_shrinker, data_writer\n",
    "\n",
    "#     valid_outputs = ['tracking', 'summary']\n",
    "#     if output not in valid_outputs:\n",
    "#         raise ValueError(f\"Invalid ouptut selection: '{output}'. Valid options are: '{valid_outputs}'\")\n",
    "\n",
    "#     try: \n",
    "#         # Transform the tracking data\n",
    "#         quant = data_loader(dataset='tracking', database='nfl_surface')\n",
    "#         quant = data_shrinker(quant)\n",
    "#         quant = angle_corrector(quant)\n",
    "#         quant = body_builder(quant, 'tracking')\n",
    "#         quant = velocity_calculator(quant)\n",
    "#         quant = impulse_calculator(quant)\n",
    "    \n",
    "#         if output == 'summary':\n",
    "#             summary = path_calculator(quant)\n",
    "#             del quant # remove the large table from memory\n",
    "#             # Open and merge the qualitative data\n",
    "#             quals = data_loader('qualitative', 'nfl_surface')\n",
    "#             qual_quant = qual_quant_merger(quals, summary)\n",
    "            \n",
    "#             print(\"Writing all quantitative and qualitative summary data to the database as summary_data. Wait.\")\n",
    "#             data_writer(qual_quant, 'nfl_surface', 'summary_data')\n",
    "#             print(\"Data has been uploaded to the database. Probably.\")        \n",
    "\n",
    "#         elif output == 'tracking':\n",
    "#             # upload the physical data to the database for machine learning\n",
    "#             print(\"Writing the transformed table with physical parameters to the database as quantitative\")\n",
    "#             data_writer(quant, 'nfl_surface', 'quantitative')\n",
    "#             print(\"Data has been uploaded to the database. Go celebrate!\")\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred with your selection, '{output}': {e}\")\n",
    "#         return None\n",
    "\n",
    "\n",
    "       \n",
    "# def transform_concussion_data(output):\n",
    "#     from DataHandler import data_loader, data_shrinker, data_writer\n",
    "\n",
    "#     valid_outputs = ['tracking', 'summary']\n",
    "#     if output not in valid_outputs:\n",
    "#         raise ValueError(f\"Invalid ouptut selection: '{output}'. Valid options are: '{valid_outputs}'\")\n",
    "\n",
    "#     try: \n",
    "#         track = data_loader(dataset='ngs_data', database='nfl_concussion')\n",
    "#         track = data_shrinker(track)\n",
    "#         track = column_corrector(track)\n",
    "#         track = angle_corrector(track)\n",
    "#         track = body_builder(track, 'ngs_data')\n",
    "#         track = velocity_calculator(track)\n",
    "#         track = impulse_calculator(track)\n",
    "    \n",
    "#         if output == 'summary':\n",
    "#             summary =  path_calculator(track)\n",
    "#             del track # remove the large table from memory\n",
    "#             # Open and merge the qualitative data\n",
    "#             quals = data_loader('qualitative', 'nfl_concussion')\n",
    "#             qual_quant = qual_quant_merger(quals, summary)\n",
    "            \n",
    "#             print(\"Writing all quantitative and qualitative summary data to the database as summary_data. Wait.\")\n",
    "#             data_writer(qual_quant, 'nfl_concussion', 'summary_data')\n",
    "#             print(\"Data has been uploaded to the database. Probably.\")        \n",
    "\n",
    "#         elif output == 'tracking':\n",
    "#             # upload the physical data to the database for machine learning\n",
    "#             print(\"Writing the transformed table with physical parameters to the database as quantitative\")\n",
    "#             data_writer(track, 'nfl_surface', 'quantitative')\n",
    "#             print(\"Data has been uploaded to the database. Good for you!\")\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred with your selection, '{output}': {e}\")\n",
    "#         return None\n",
    "\n",
    "\n",
    "\n",
    "# def transform_concussion_data():\n",
    "#     from DataHandler import data_loader, data_shrinker, data_writer\n",
    "\n",
    "#     track = data_loader(dataset='ngs_data', database='nfl_concussion')\n",
    "#     track = data_shrinker(track)\n",
    "#     track = column_corrector(track)\n",
    "#     track = angle_corrector(track)\n",
    "#     track = body_builder(track, 'ngs_data')\n",
    "#     track = velocity_calculator(track)\n",
    "#     track = impulse_calculator(track)\n",
    "#     summary = path_calculator(track)\n",
    "#     # transform the ngs_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #############################################\n",
    "# def column_corrector(df):\n",
    "#     import polars as pl\n",
    "#     \"\"\"\n",
    "#     Add a Play_Time column that acts like the 'time' column did in the injury dataset. \n",
    "#     Each PlayKey will start at 0.0 and increase by 0.1 for each subsequent record.\n",
    "#     \"\"\"\n",
    "#     df = df.with_columns([\n",
    "#         pl.concat_str([\n",
    "#             pl.col('gsisid').cast(pl.Int32).cast(pl.Utf8)\n",
    "#             , pl.lit('-')\n",
    "#             , pl.col('gamekey').cast(pl.Utf8)\n",
    "#             , pl.lit('-')\n",
    "#             , pl.col('playid').cast(pl.Utf8)\n",
    "#         ]).alias('PlayKey')\n",
    "#     ])\n",
    "     \n",
    "    \n",
    "#     df = df.select([\n",
    "#         'PlayKey'\n",
    "#         , 'time'\n",
    "#         , 'x'\n",
    "#         , 'y'\n",
    "#         , 'o'\n",
    "#         , 'dir'\n",
    "#         , 'gsisid'\n",
    "#         ]).rename({\"time\":\"datetime\"})\n",
    "\n",
    "#     df = df.sort(['PlayKey', 'datetime'])\n",
    "\n",
    "#     df = df.with_columns(\n",
    "#         (pl.arange(0, pl.len()) * 0.1).over(\"PlayKey\").alias(\"time\")\n",
    "#         ).with_columns([pl.col('gsisid').cast(pl.Int32)])  \n",
    "    \n",
    "#     return df\n",
    "\n",
    "\n",
    "\n",
    "# def calculate_angle_difference(angle1, angle2):\n",
    "#     import numpy as np\n",
    "#     \"\"\"\n",
    "#     Calculate the smallest angle difference between two angles \n",
    "#     using trigonometric functions, accounting for edge cases.\n",
    "#     \"\"\"\n",
    "#     sin_diff = np.sin(np.radians(angle2 - angle1))\n",
    "#     cos_diff = np.cos(np.radians(angle2 - angle1))\n",
    "#     return np.degrees(np.arctan2(sin_diff, cos_diff))\n",
    "\n",
    "# def angle_corrector(df):\n",
    "#     import polars as pl\n",
    "#     \"\"\"\n",
    "#     Make corrections to angles to reduce fringe errors at 360\n",
    "#     \"\"\"\n",
    "#     df = df.with_columns([\n",
    "#         ((pl.col(\"dir\") + 180) % 360 - 180).alias(\"dir\")\n",
    "#         , ((pl.col(\"o\") + 180) % 360 - 180).alias(\"o\")\n",
    "#     ]).with_columns(\n",
    "#         (calculate_angle_difference(pl.col(\"dir\"), pl.col(\"o\"))).abs().round(2).alias(\"Angle_Diff\")\n",
    "#         )\n",
    "    \n",
    "#     return df\n",
    "\n",
    "\n",
    "# def body_builder(df, df_name):\n",
    "#     body_data = pl.DataFrame({\n",
    "#         \"position\": [\"QB\", \"RB\", \"FB\", \"WR\", \"TE\", \"T\", \"G\", \"C\", \"DE\", \"DT\", \"NT\", \"LB\", \"OLB\", \"MLB\", \"CB\", \"S\", \"K\", \"P\", \"SS\", \"ILB\", \"FS\", \"LS\", \"DB\"]\n",
    "#         # , \"Position_Name\": [\"Quarterback\", \"Running Back\", \"Fullback\", \"Wide Receiver\", \"Tight End\", \"Tackle\", \"Guard\", \"Center\", \"Defensive End\", \"Defensive Tackle\", \"Nose Tackle\", \"Linebacker\", \"Outside Linebacker\", \"Middle Linebacker\", \"Cornerback\", \"Safety\", \"Kicker\", \"Punter\", \"Strong Safety\", \"Inside Linebacker\", \"Free Safety\", \"Long Snapper\", \"Defensive Back\"]\n",
    "#         , \"Height_m\": [1.91, 1.79, 1.85, 1.88, 1.96, 1.97, 1.90, 1.87, 1.97, 1.92, 1.88, 1.90, 1.90, 1.87, 1.82, 1.84, 1.83, 1.88, 1.84, 1.90, 1.84, 1.88, 1.82]\n",
    "#         , \"Weight_kg\": [102.1, 95.3, 111.1, 90.7, 114.6, 140.6, 141.8, 136.1, 120.2, 141.8, 152.0, 110.0, 108.9, 113.4, 87.4, 95.9, 92.08, 97.52, 95.9, 110.0, 95.9, 108.86, 87.4]\n",
    "#         , \"Chest_rad_m\": [0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191]\n",
    "#         })\n",
    "\n",
    "#     valid_df_names = ['ngs_data', 'tracking']\n",
    "#     if df_name not in valid_df_names:\n",
    "#         raise ValueError(f\"Invalid dataframe name '{df_name}'. Valid options are: {valid_df_names}\")\n",
    "\n",
    "#     try: \n",
    "#         if df_name == 'ngs_data':\n",
    "#             position = data_loader(dataset='positions', database='nfl_concussion')\n",
    "#             position = position.join(\n",
    "#                 body_data\n",
    "#                 , left_on='position'\n",
    "#                 , right_on='position'\n",
    "#                 , how='left'\n",
    "#                 )\n",
    "            \n",
    "#             df = df.join(\n",
    "#                 position\n",
    "#                 , on='gsisid'\n",
    "#                 , how='left'\n",
    "#                 ).drop_nulls(subset=['position'])\n",
    "            \n",
    "\n",
    "#         elif df_name == 'tracking':\n",
    "#             position = data_loader(dataset='play_positions', database='nfl_surface')\n",
    "#             position = position.join(\n",
    "#                 body_data\n",
    "#                 , left_on='position'\n",
    "#                 , right_on='position'\n",
    "#                 , how='left'\n",
    "#                 )\n",
    "\n",
    "#             df = df.join(\n",
    "#                 position\n",
    "#                 , left_on='PlayKey'\n",
    "#                 , right_on='playkey'\n",
    "#                 , how='left'\n",
    "#             ).drop_nulls(subset=['position']).drop(['event'])\n",
    "\n",
    "            \n",
    "\n",
    "#         return df    \n",
    "    \n",
    "#     except Exception as e: \n",
    "#         print(f\"An error occurred while loading the dataframe '{df_name}': {e}\")\n",
    "#         return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def velocity_calculator(df):\n",
    "#     import numpy as np\n",
    "#     import polars as pl\n",
    "#     \"\"\"\n",
    "#     Using the (X,Y) and time columns, perform calculations based on the \n",
    "#     difference between two rows to find displacement, speed, direction \n",
    "#     of motion, velocity in x and y components, and the angular velocities \n",
    "#     of the direction of motion and orientations \n",
    "#     \"\"\"\n",
    "    \n",
    "#     return df.with_columns([\n",
    "#         # Convert 'o' and 'dir' to radians\n",
    "#         (pl.col(\"o\") * np.pi / 180).alias(\"o_rad\"),\n",
    "#         (pl.col(\"dir\") * np.pi / 180).alias(\"dir_rad\")\n",
    "#     ]).with_columns([\n",
    "#         # Pre-calculate shifted values\n",
    "#         pl.col(\"x\").shift(1).over(\"PlayKey\").alias(\"prev_x\")\n",
    "#         , pl.col(\"y\").shift(1).over(\"PlayKey\").alias(\"prev_y\")\n",
    "#         # , pl.col(\"time\").shift(1).over(\"PlayKey\").alias(\"prev_time\")\n",
    "#         , pl.col(\"dir_rad\").shift(1).over(\"PlayKey\").alias(\"prev_dir\")\n",
    "#         , pl.col(\"o_rad\").shift(1).over(\"PlayKey\").alias(\"prev_o\")\n",
    "#     ]).with_columns([\n",
    "#         # Calculate the component displacements \n",
    "#           (pl.col(\"x\") - pl.col(\"prev_x\")).alias(\"dx\")\n",
    "#         , (pl.col(\"y\") - pl.col(\"prev_y\")).alias(\"dy\")\n",
    "#     ]).with_columns([\n",
    "#         # Calculate displacement\n",
    "#         ((pl.col(\"dx\")**2 + pl.col(\"dy\")**2)**0.5).alias(\"Displacement\")\n",
    "#     ]).with_columns([\n",
    "#         # Calculate speed\n",
    "#         (pl.col(\"Displacement\") / 0.1).alias(\"Speed\")\n",
    "#         # Calculate direction\n",
    "#         , (np.degrees(np.arctan2(pl.col(\"dx\"), pl.col(\"dy\")))).alias(\"Direction\")\n",
    "#         # Calculate velocity components\n",
    "#         , (pl.col(\"dx\") / 0.1).alias(\"vx\")\n",
    "#         , (pl.col(\"dy\") / 0.1).alias(\"vy\")\n",
    "#         # Calculate angular velocities\n",
    "#         , ((pl.col(\"dir_rad\") - pl.col(\"prev_dir\")) / 0.1).alias(\"omega_dir\")\n",
    "#         , ((pl.col(\"o_rad\") - pl.col(\"prev_o\")) / 0.1).alias(\"omega_o\")\n",
    "#     ]).with_columns([\n",
    "#         ((pl.col(\"omega_dir\") - pl.col(\"omega_o\")).abs()).alias(\"omega_diff\")\n",
    "#     ]).drop([\n",
    "#         \"prev_x\", \"prev_y\", \"prev_dir\", \"prev_o\", \"dx\", \"dy\", \"o_rad\", \"dir_rad\"\n",
    "#     ])\n",
    "\n",
    "\n",
    "# def impulse_calculator(df):\n",
    "#     import numpy as np\n",
    "#     import polars as pl\n",
    "#     \"\"\"\n",
    "#     Using the (X,Y) and time columns, perform calculations based on the velocities and changes \n",
    "#     in velocites along with player mass to get the momentum and impulse, a measure that can \n",
    "#     be assessed along with medical data related to concussions and injuries\n",
    "#     \"\"\"\n",
    "    \n",
    "#     return df.with_columns([\n",
    "#         # Calculate the linear momentum for each instant\n",
    "#         (pl.col('vx') * pl.col('Weight_kg')).alias('px')\n",
    "#         , (pl.col('vy') * pl.col('Weight_kg')).alias('py')\n",
    "\n",
    "#         # Calculate the moment of inertia of a rotating upright body (1/12 mr^2)\n",
    "#         , (1/12 * pl.col('Weight_kg') * (pl.col('Chest_rad_m')**2)).alias('moment')\n",
    "        \n",
    "#         # Calculate the moment of inertia of the upper body turning upright with respect to waist (70% mass)\n",
    "#         , (1/12 * (pl.col('Weight_kg')*0.7) * (pl.col('Chest_rad_m')**2)).alias('moment_upper')\n",
    "    \n",
    "#     ]).with_columns([\n",
    "#           # Calculate the magnitude of linear momentum\n",
    "#         ((pl.col(\"px\")**2 + pl.col(\"py\")**2)**0.5).alias(\"p_magnitude\")\n",
    "        \n",
    "#         # Calculate the angular momentum for the direction\n",
    "#         , (pl.col('omega_dir')*pl.col('moment')).alias('L_dir')\n",
    "\n",
    "#         # Calculate the angular momentum of the upper body with respect to lower\n",
    "#         , (pl.col('omega_diff')*pl.col('moment_upper')).alias('L_diff')\n",
    "\n",
    "\n",
    "#     ]).with_columns([\n",
    "#         # Pre-calculate shifted values for linear and angular momenta\n",
    "#         pl.col(\"px\").shift(1).over(\"PlayKey\").alias(\"prev_px\")\n",
    "#         , pl.col(\"py\").shift(1).over(\"PlayKey\").alias(\"prev_py\")\n",
    "#         , pl.col(\"L_dir\").shift(1).over(\"PlayKey\").alias(\"prev_L_dir\")\n",
    "#         , pl.col(\"L_diff\").shift(1).over(\"PlayKey\").alias(\"prev_L_diff\")\n",
    "        \n",
    "#     ]).with_columns([\n",
    "#         # Calculate impulse, J, which is the change in linear momentum \n",
    "#         ((pl.col(\"px\") - pl.col(\"prev_px\"))).alias(\"Jx\")\n",
    "#         , ((pl.col(\"py\") - pl.col(\"prev_py\"))).alias(\"Jy\")\n",
    "        \n",
    "#     ]).with_columns([\n",
    "#           # Calculate the magnitude of linear momentum\n",
    "#         ((pl.col(\"Jx\")**2 + pl.col(\"Jy\")**2)**0.5).alias(\"J_magnitude\")\n",
    "\n",
    "#         # Calculate torque as the change in angular momentum L over the change in time\n",
    "#         , (((pl.col(\"L_dir\") - pl.col(\"prev_L_dir\"))) / 0.1).alias(\"torque\")\n",
    "#         , (((pl.col(\"L_diff\") - pl.col(\"prev_L_diff\"))) / 0.1).alias(\"torque_internal\")\n",
    "\n",
    "#     ]).drop([\n",
    "#         \"prev_L_dir\", \"prev_px\", \"prev_py\", \"prev_L_diff\"\n",
    "#     ])\n",
    "\n",
    "\n",
    "# def path_calculator(df):\n",
    "#     import polars as pl\n",
    "#     # This provides a summary table that can be integrated with the qualitative data\n",
    "\n",
    "#     # Calculate total distance and displacement for each PlayKey\n",
    "#     # Calculate total distance and displacement for each PlayKey\n",
    "#     result = df.select([\n",
    "#         \"PlayKey\"\n",
    "#         , pl.col(\"Displacement\").sum().over(\"PlayKey\").alias(\"Distance\")\n",
    "#         , pl.col(\"x\").first().over(\"PlayKey\").alias(\"start_x\")\n",
    "#         , pl.col(\"y\").first().over(\"PlayKey\").alias(\"start_y\")\n",
    "#         , pl.col(\"x\").last().over(\"PlayKey\").alias(\"end_x\")\n",
    "#         , pl.col(\"y\").last().over(\"PlayKey\").alias(\"end_y\")\n",
    "#         , pl.col(\"Angle_Diff\").max().over(\"PlayKey\").alias(\"Max_Angle_Diff\")\n",
    "#         , pl.col(\"Angle_Diff\").mean().over(\"PlayKey\").alias(\"Mean_Angle_Diff\")\n",
    "#         , pl.col(\"Speed\").max().over(\"PlayKey\").alias(\"Max_Speed\")\n",
    "#         , pl.col(\"Speed\").mean().over(\"PlayKey\").alias(\"Mean_Speed\")\n",
    "#         , pl.col(\"J_magnitude\").max().over(\"PlayKey\").alias(\"Max_Impulse\")\n",
    "#         , pl.col(\"J_magnitude\").mean().over(\"PlayKey\").alias(\"Mean_Impulse\")\n",
    "#         , pl.col(\"torque\").max().over(\"PlayKey\").alias(\"Max_Torque\")\n",
    "#         , pl.col(\"torque\").mean().over(\"PlayKey\").alias(\"Mean_Torque\")\n",
    "#         , pl.col(\"torque_internal\").max().over(\"PlayKey\").alias(\"Max_Int_Torque\")\n",
    "#         , pl.col(\"torque_internal\").mean().over(\"PlayKey\").alias(\"Mean_Int_Torque\")\n",
    "\n",
    "#         ]).unique(subset=[\"PlayKey\"])\n",
    "\n",
    "\n",
    "#     # Calculate the displacement\n",
    "#     result = result.with_columns([\n",
    "#         (((pl.col(\"end_x\") - pl.col(\"start_x\"))**2 + \n",
    "#           (pl.col(\"end_y\") - pl.col(\"start_y\"))**2)**0.5)\n",
    "#         .alias(\"Displacement\")\n",
    "#         ]).with_columns([\n",
    "#             (pl.col(\"Distance\") - pl.col(\"Displacement\")).alias(\"Path_Diff\")\n",
    "#         ])\n",
    "\n",
    "     \n",
    "#     # Select only the required columns\n",
    "#     result = result.select([\n",
    "#         'PlayKey'\n",
    "#         , 'Distance'\n",
    "#         , 'Displacement'\n",
    "#         , 'Path_Diff'\n",
    "#         , 'Max_Angle_Diff'\n",
    "#         , 'Mean_Angle_Diff'\n",
    "#         , 'Max_Speed'\n",
    "#         , 'Mean_Speed'\n",
    "#         , 'Max_Impulse'\n",
    "#         , 'Mean_Impulse'\n",
    "#         , 'Max_Torque'\n",
    "#         , 'Mean_Torque'\n",
    "#         , 'Max_Int_Torque'\n",
    "#         , 'Mean_Int_Torque'\n",
    "      \n",
    "#     ]).sort(\"PlayKey\")\n",
    "\n",
    "\n",
    "#     return result\n",
    "\n",
    "# # Join the Qualitative with the Quantitative to create Summary Table\n",
    "# def qual_quant_merger(quals, quant):\n",
    "#     from DataHandler import data_shrinker\n",
    "#     qual_quant = quals.join(quant, on=\"PlayKey\", how=\"left\")\n",
    "#     qual_quant = data_shrinker(qual_quant)\n",
    "\n",
    "#     return qual_quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
