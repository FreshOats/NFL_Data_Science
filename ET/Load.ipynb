{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Clean Files to Database\n",
    "\n",
    "There are 3 files that need to be loaded to the database, NFL. The database will need to be created and tables defined before loading. \n",
    "A few things of note: \n",
    "1. I will be using SQL Server this time, which means that TINYINT is unsigned, and ranges from 0 to 255 instead of -127 to 127. This will \n",
    "have implications on any negative values, in which case, SMALLINT will have to be used. \n",
    "2. The tracking data files will be concatenated, since they include only the plays involving injuries.\n",
    "3. The machine learning datasets will be separated into Concussions and Injuries, since there are data from each that would make the tables \n",
    "excessively large filled with null values per analysis. Additionally, there is only one play type for the concussion set - punt plays, which is \n",
    "only a minor subset of the injuries list.\n",
    "\n",
    "I will be using SQLAlchemy to push and pull the data to and from the database. While I have been using polars for this analysis, SQLAlchemy \n",
    "cannot read a parquet or polars dataframe; it can only read a pandas formatted dataframe. Thus, there will be a single step before the write \n",
    "process that converts to a pandas df prior to appending the data. \n",
    "\n",
    "Up to this point, I have been reading the files in directly, however, since this is intended to be a pipline, the clean data will be pulled, as \n",
    "the first step, from the database to maintain data fidelity and reduce chances of data corruption. \n",
    "\n",
    "\n",
    "***\n",
    "#### All Files Now\n",
    "\n",
    "Currently, the files include: \n",
    "- All_Tracking - This contains time, position, physics, per play key including all injuries and concussions including opponent key when needed\n",
    "- Full_Summary_Concussions - Contains all of the summary data in addition to the descriptive stats of the concussion plays both with and without injuries per playkey\n",
    "- Full_Summary_Injuries - Same as above, only not concussion but injury set, including the IsSevere column\n",
    "- OpponentPlays - Temp file, adding opponent PlayKeys to the Concussion Playkeys\n",
    "- OptimizedTrackData - Temp File, Very large file from the Injuries Data, served as a temp file\n",
    "- QualitativeConcussions - Temp File, qualitative collected data prior to adding to All Tracking\n",
    "- QualitativeInjuries - Temp File, qualitiative collected prior to adding to All Tracking\n",
    "- TrackingInjuries - Temp File, injury equivalent of OpponentPlays\n",
    "\n",
    "I need to determine what each of these files represents, and then I need to determine which of them needs to be loaded to a database prior to machine learning and viz production\n",
    "\n",
    "\n",
    "#### Loading: \n",
    "\n",
    "I will be loading All_Tracking for the Vizzes\n",
    "Additionally, I will be loading Full_Summary_Concussions and Full_Summary_Injuries for the machine learning applications. \n",
    "In the case that I want to include different surface types or other parameters to the vizzes, I can do a join and then output those data if necessary. For now, there's no need to increase the size of the data\n",
    "\n",
    "***\n",
    "\n",
    "### Creating the Database and Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE DATABASE NFL;\n",
    "GO\n",
    "\n",
    "USE NFL;\n",
    "GO\n",
    "\n",
    "CREATE TABLE Tracking_Data (\n",
    "    PlayKey VARCHAR(20),\n",
    "    time FLOAT,\n",
    "    x FLOAT,\n",
    "    y FLOAT,\n",
    "    dir FLOAT,\n",
    "    o FLOAT,\n",
    "    Angle_Diff FLOAT,\n",
    "    Displacement FLOAT,\n",
    "    Speed FLOAT,\n",
    "    vx FLOAT,\n",
    "    vy FLOAT,\n",
    "    omega_dir FLOAT,\n",
    "    omega_o FLOAT,\n",
    "    omega_diff FLOAT,\n",
    "    Position VARCHAR(50),\n",
    "    Height_m FLOAT,\n",
    "    Weight_kg FLOAT,\n",
    "    Chest_rad_m FLOAT,\n",
    "    px FLOAT,\n",
    "    py FLOAT,\n",
    "    moment FLOAT,\n",
    "    moment_upper FLOAT,\n",
    "    p_magnitude FLOAT,\n",
    "    L_dir FLOAT,\n",
    "    L_diff FLOAT,\n",
    "    Jx FLOAT,\n",
    "    Jy FLOAT,\n",
    "    J_magnitude FLOAT,\n",
    "    torque FLOAT,\n",
    "    torque_internal FLOAT,\n",
    "    InjuryType VARCHAR(50),\n",
    "    GSISID INT,\n",
    "    Player_Activity_Derived VARCHAR(50),\n",
    "    Primary_Impact_Type VARCHAR(50),\n",
    "    Primary_Partner_GSISID VARCHAR(20),\n",
    "    Primary_Partner_Activity_Derived VARCHAR(50),\n",
    "    OpponentKey VARCHAR(20)\n",
    ");\n",
    "\n",
    "CREATE TABLE Concussion_Summary (\n",
    "    PlayKey VARCHAR(20),\n",
    "    Position VARCHAR(50),\n",
    "    Role VARCHAR(50),\n",
    "    Play_Type VARCHAR(50),\n",
    "    Poss_Team VARCHAR(50),\n",
    "    Game_Site VARCHAR(50),\n",
    "    HomeTeamCode VARCHAR(50),\n",
    "    VisitTeamCode VARCHAR(50),\n",
    "    StadiumType VARCHAR(50),\n",
    "    FieldType VARCHAR(50),\n",
    "    Weather VARCHAR(50),\n",
    "    Temperature FLOAT,\n",
    "    Player_Activity_Derived VARCHAR(50),\n",
    "    Primary_Impact_Type VARCHAR(50),\n",
    "    Primary_Partner_Activity_Derived VARCHAR(50),\n",
    "    Primary_Partner_GSISID INT,\n",
    "    OpponentKey VARCHAR(20),\n",
    "    IsInjured TINYINT,\n",
    "    Home_Score TINYINT,\n",
    "    Visiting_Score TINYINT,\n",
    "    Score_Difference TINYINT,\n",
    "    Position_right VARCHAR(50),\n",
    "    Distance FLOAT,\n",
    "    Displacement FLOAT,\n",
    "    Path_Diff FLOAT,\n",
    "    Max_Angle_Diff FLOAT,\n",
    "    Mean_Angle_Diff FLOAT,\n",
    "    Max_Speed FLOAT,\n",
    "    Mean_Speed FLOAT,\n",
    "    Max_Impulse FLOAT,\n",
    "    Mean_Impulse FLOAT,\n",
    "    Max_Torque FLOAT,\n",
    "    Mean_Torque FLOAT,\n",
    "    Max_Int_Torque FLOAT,\n",
    "    Mean_Int_Torque FLOAT\n",
    ");\n",
    "\n",
    "CREATE TABLE Injury_Summary (\n",
    "    PlayKey VARCHAR(20),\n",
    "    Position VARCHAR(50),\n",
    "    StadiumType VARCHAR(50),\n",
    "    FieldType VARCHAR(50),\n",
    "    Temperature SMALLINT,\n",
    "    Weather VARCHAR(50),\n",
    "    PlayType VARCHAR(50),\n",
    "    BodyPart VARCHAR(50),\n",
    "    DM_M1 TINYINT,\n",
    "    DM_M7 TINYINT,\n",
    "    DM_M28 TINYINT,\n",
    "    DM_M42 TINYINT,\n",
    "    IsInjured TINYINT,\n",
    "    IsSevere TINYINT,\n",
    "    Position_right VARCHAR(50),\n",
    "    Distance FLOAT,\n",
    "    Displacement FLOAT,\n",
    "    Path_Diff FLOAT,\n",
    "    Max_Angle_Diff FLOAT,\n",
    "    Mean_Angle_Diff FLOAT,\n",
    "    Max_Speed FLOAT,\n",
    "    Mean_Speed FLOAT,\n",
    "    Max_Impulse FLOAT,\n",
    "    Mean_Impulse FLOAT,\n",
    "    Max_Torque FLOAT,\n",
    "    Mean_Torque FLOAT,\n",
    "    Max_Int_Torque FLOAT,\n",
    "    Mean_Int_Torque FLOAT\n",
    ");\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populating the Tables in the Database using SQLAlchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from sqlalchemy import create_engine\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "\n",
    "path = \"F:/Data/Processing_data/\"\n",
    "file1 = \"All_Tracking.parquet\"\n",
    "file2 = \"Full_Summary_Concussions.parquet\"\n",
    "file3 = \"Full_Summary_Injuries.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert dictionary columns with uint32 indices\n",
    "def convert_uint32_dict_columns(table):\n",
    "    new_columns = []\n",
    "    for col in table.columns:\n",
    "        if pa.types.is_dictionary(col.type) and col.type.index_type == pa.uint32():\n",
    "            new_col = col.cast(pa.string())\n",
    "        else:\n",
    "            new_col = col\n",
    "        new_columns.append(new_col)\n",
    "    return pa.Table.from_arrays(new_columns, names=table.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Parquet file using Polars\n",
    "df = pl.read_parquet(os.path.join(path, file1))\n",
    "\n",
    "# Create SQLAlchemy engine\n",
    "engine = create_engine('mssql+pyodbc:///?odbc_connect=DRIVER={ODBC Driver 17 for SQL Server};SERVER=GOOSEBOX;DATABASE=NFL;Trusted_Connection=yes;')\n",
    "\n",
    "# Convert Polars DataFrame to PyArrow Table\n",
    "arrow_table = df.to_arrow()\n",
    "\n",
    "# Convert problematic columns\n",
    "converted_table = convert_uint32_dict_columns(arrow_table)\n",
    "\n",
    "# Use PyArrow to write to SQL\n",
    "with engine.connect() as connection:\n",
    "    # Convert PyArrow table to pandas DataFrame\n",
    "    pandas_df = converted_table.to_pandas()\n",
    "    \n",
    "    # Write to SQL\n",
    "    pandas_df.to_sql('Tracking_Data', connection, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Parquet file using Polars\n",
    "df = pl.read_parquet(os.path.join(path, file2))\n",
    "\n",
    "# Convert Polars DataFrame to PyArrow Table\n",
    "arrow_table = df.to_arrow()\n",
    "\n",
    "# Convert problematic columns\n",
    "converted_table = convert_uint32_dict_columns(arrow_table)\n",
    "\n",
    "# Use PyArrow to write to SQL\n",
    "with engine.connect() as connection:\n",
    "    # Convert PyArrow table to pandas DataFrame\n",
    "    pandas_df = converted_table.to_pandas()\n",
    "    \n",
    "    # Write to SQL\n",
    "    pandas_df.to_sql('Concussion_Summary', connection, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Parquet file using Polars\n",
    "df = pl.read_parquet(os.path.join(path, file3))\n",
    "\n",
    "# Convert Polars DataFrame to PyArrow Table\n",
    "arrow_table = df.to_arrow()\n",
    "\n",
    "# Convert problematic columns\n",
    "converted_table = convert_uint32_dict_columns(arrow_table)\n",
    "\n",
    "# Use PyArrow to write to SQL\n",
    "with engine.connect() as connection:\n",
    "    # Convert PyArrow table to pandas DataFrame\n",
    "    pandas_df = converted_table.to_pandas()\n",
    "    \n",
    "    # Write to SQL\n",
    "    pandas_df.to_sql('Injury_Summary', connection, if_exists='append', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
