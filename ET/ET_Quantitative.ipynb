{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract and Transform the Injury and Concussion Tracking Data\n",
    "\n",
    "This data is much more voluminous than the qualitative data. Maintaining the datatypes is paramount to keeping the size smaller. Again, this will be the set of functions for all of the transformation processing of the tracking data and the listing of the .py files used to actually process and save the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_angle_difference(angle1, angle2):\n",
    "    import numpy as np\n",
    "    \"\"\"\n",
    "    Calculate the smallest angle difference between two angles \n",
    "    using trigonometric functions, accounting for edge cases.\n",
    "    \"\"\"\n",
    "    sin_diff = np.sin(np.radians(angle2 - angle1))\n",
    "    cos_diff = np.cos(np.radians(angle2 - angle1))\n",
    "    return np.degrees(np.arctan2(sin_diff, cos_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def angle_corrector(df):\n",
    "    import polars as pl\n",
    "    \"\"\"\n",
    "    Make corrections to angles to reduce fringe errors at 360\n",
    "    \"\"\"\n",
    "    df = df.with_columns([\n",
    "        ((pl.col(\"dir\") + 180) % 360 - 180).alias(\"dir\")\n",
    "        , ((pl.col(\"o\") + 180) % 360 - 180).alias(\"o\")\n",
    "    ]).with_columns(\n",
    "        (calculate_angle_difference(pl.col(\"dir\"), pl.col(\"o\"))).abs().round(2).alias(\"Angle_Diff\")\n",
    "        )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_calculator(df):\n",
    "    import polars as pl # type: ignore\n",
    "    # This provides a summary table that can be integrated with the qualitative data\n",
    "\n",
    "    # Calculate total distance and displacement for each PlayKey\n",
    "    # Calculate total distance and displacement for each PlayKey\n",
    "    result = df.select([\n",
    "        \"PlayKey\"\n",
    "        , pl.col(\"Displacement\").sum().over(\"PlayKey\").alias(\"Distance\")\n",
    "        , pl.col(\"x\").first().over(\"PlayKey\").alias(\"start_x\")\n",
    "        , pl.col(\"y\").first().over(\"PlayKey\").alias(\"start_y\")\n",
    "        , pl.col(\"x\").last().over(\"PlayKey\").alias(\"end_x\")\n",
    "        , pl.col(\"y\").last().over(\"PlayKey\").alias(\"end_y\")\n",
    "        , pl.col(\"Angle_Diff\").max().over(\"PlayKey\").alias(\"Max_Angle_Diff\")\n",
    "        , pl.col(\"Angle_Diff\").mean().over(\"PlayKey\").alias(\"Mean_Angle_Diff\")\n",
    "        , pl.col(\"Speed\").max().over(\"PlayKey\").alias(\"Max_Speed\")\n",
    "        , pl.col(\"Speed\").mean().over(\"PlayKey\").alias(\"Mean_Speed\")\n",
    "        , pl.col(\"J_magnitude\").max().over(\"PlayKey\").alias(\"Max_Impulse\")\n",
    "        , pl.col(\"J_magnitude\").mean().over(\"PlayKey\").alias(\"Mean_Impulse\")\n",
    "        , pl.col(\"torque\").max().over(\"PlayKey\").alias(\"Max_Torque\")\n",
    "        , pl.col(\"torque\").mean().over(\"PlayKey\").alias(\"Mean_Torque\")\n",
    "        , pl.col(\"torque_internal\").max().over(\"PlayKey\").alias(\"Max_Int_Torque\")\n",
    "        , pl.col(\"torque_internal\").mean().over(\"PlayKey\").alias(\"Mean_Int_Torque\")\n",
    "\n",
    "        ]).unique(subset=[\"PlayKey\"])\n",
    "\n",
    "\n",
    "    # Calculate the displacement\n",
    "    result = result.with_columns([\n",
    "        (((pl.col(\"end_x\") - pl.col(\"start_x\"))**2 + \n",
    "          (pl.col(\"end_y\") - pl.col(\"start_y\"))**2)**0.5)\n",
    "        .alias(\"Displacement\")\n",
    "        ]).with_columns([\n",
    "            (pl.col(\"Distance\") - pl.col(\"Displacement\")).alias(\"Path_Diff\")\n",
    "        ])\n",
    "\n",
    "     \n",
    "    # Select only the required columns\n",
    "    result = result.select([\n",
    "        'PlayKey'\n",
    "        , 'Distance'\n",
    "        , 'Displacement'\n",
    "        , 'Path_Diff'\n",
    "        , 'Max_Angle_Diff'\n",
    "        , 'Mean_Angle_Diff'\n",
    "        , 'Max_Speed'\n",
    "        , 'Mean_Speed'\n",
    "        , 'Max_Impulse'\n",
    "        , 'Mean_Impulse'\n",
    "        , 'Max_Torque'\n",
    "        , 'Mean_Torque'\n",
    "        , 'Max_Int_Torque'\n",
    "        , 'Mean_Int_Torque'\n",
    "      \n",
    "    ]).sort(\"PlayKey\")\n",
    "\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_corrector(df):\n",
    "    import polars as pl # type: ignore\n",
    "    \"\"\"\n",
    "    Add a Play_Time column that acts like the 'time' column did in the injury dataset. \n",
    "    Each PlayKey will start at 0.0 and increase by 0.1 for each subsequent record.\n",
    "    \"\"\"\n",
    "    df = df.with_columns([\n",
    "        pl.concat_str([\n",
    "            pl.col('gsisid').cast(pl.Int32).cast(pl.Utf8)\n",
    "            , pl.lit('-')\n",
    "            , pl.col('gamekey').cast(pl.Utf8)\n",
    "            , pl.lit('-')\n",
    "            , pl.col('playid').cast(pl.Utf8)\n",
    "        ]).alias('PlayKey')\n",
    "    ])\n",
    "     \n",
    "    \n",
    "    df = df.select([\n",
    "        'PlayKey'\n",
    "        , 'time'\n",
    "        , 'x'\n",
    "        , 'y'\n",
    "        , 'o'\n",
    "        , 'dir'\n",
    "        , 'gsisid'\n",
    "        ]).rename({\"time\":\"datetime\"})\n",
    "\n",
    "    df = df.sort(['PlayKey', 'datetime'])\n",
    "\n",
    "    df = df.with_columns(\n",
    "        (pl.arange(0, pl.len()) * 0.1).over(\"PlayKey\").alias(\"time\")\n",
    "        ).with_columns([pl.col('gsisid').cast(pl.Int32)])  \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def body_builder(df, df_name):\n",
    "    import polars as pl # type: ignore\n",
    "    from DataHandler import data_loader\n",
    "\n",
    "    body_data = pl.DataFrame({\n",
    "        \"position\": [\"QB\", \"RB\", \"FB\", \"WR\", \"TE\", \"T\", \"G\", \"C\", \"DE\", \"DT\", \"NT\", \"LB\", \"OLB\", \"MLB\", \"CB\", \"S\", \"K\", \"P\", \"SS\", \"ILB\", \"FS\", \"LS\", \"DB\"]\n",
    "        # , \"Position_Name\": [\"Quarterback\", \"Running Back\", \"Fullback\", \"Wide Receiver\", \"Tight End\", \"Tackle\", \"Guard\", \"Center\", \"Defensive End\", \"Defensive Tackle\", \"Nose Tackle\", \"Linebacker\", \"Outside Linebacker\", \"Middle Linebacker\", \"Cornerback\", \"Safety\", \"Kicker\", \"Punter\", \"Strong Safety\", \"Inside Linebacker\", \"Free Safety\", \"Long Snapper\", \"Defensive Back\"]\n",
    "        , \"Height_m\": [1.91, 1.79, 1.85, 1.88, 1.96, 1.97, 1.90, 1.87, 1.97, 1.92, 1.88, 1.90, 1.90, 1.87, 1.82, 1.84, 1.83, 1.88, 1.84, 1.90, 1.84, 1.88, 1.82]\n",
    "        , \"Weight_kg\": [102.1, 95.3, 111.1, 90.7, 114.6, 140.6, 141.8, 136.1, 120.2, 141.8, 152.0, 110.0, 108.9, 113.4, 87.4, 95.9, 92.08, 97.52, 95.9, 110.0, 95.9, 108.86, 87.4]\n",
    "        , \"Chest_rad_m\": [0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191]\n",
    "        })\n",
    "\n",
    "    valid_df_names = ['ngs_data', 'tracking']\n",
    "    if df_name not in valid_df_names:\n",
    "        raise ValueError(f\"Invalid dataframe name '{df_name}'. Valid options are: {valid_df_names}\")\n",
    "\n",
    "    try: \n",
    "        if df_name == 'ngs_data':\n",
    "            position = data_loader(dataset='positions', database='nfl_concussion')\n",
    "            position = position.join(\n",
    "                body_data\n",
    "                , left_on='position'\n",
    "                , right_on='position'\n",
    "                , how='left'\n",
    "                )\n",
    "            \n",
    "            df = df.join(\n",
    "                position\n",
    "                , on='gsisid'\n",
    "                , how='left'\n",
    "                ).drop_nulls(subset=['position'])\n",
    "            \n",
    "\n",
    "        elif df_name == 'tracking':\n",
    "            position = data_loader(dataset='play_positions', database='nfl_surface')\n",
    "            position = position.join(\n",
    "                body_data\n",
    "                , left_on='position'\n",
    "                , right_on='position'\n",
    "                , how='left'\n",
    "                )\n",
    "\n",
    "            df = df.join(\n",
    "                position\n",
    "                , left_on='PlayKey'\n",
    "                , right_on='playkey'\n",
    "                , how='left'\n",
    "            ).drop_nulls(subset=['position']).drop(['event'])\n",
    "\n",
    "            \n",
    "\n",
    "        return df    \n",
    "    \n",
    "    except Exception as e: \n",
    "        print(f\"An error occurred while loading the dataframe '{df_name}': {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def velocity_calculator(df):\n",
    "    import numpy as np # type: ignore\n",
    "    import polars as pl # type: ignore\n",
    "    \"\"\"\n",
    "    Using the (X,Y) and time columns, perform calculations based on the \n",
    "    difference between two rows to find displacement, speed, direction \n",
    "    of motion, velocity in x and y components, and the angular velocities \n",
    "    of the direction of motion and orientations \n",
    "    \"\"\"\n",
    "    \n",
    "    return df.with_columns([\n",
    "        # Convert 'o' and 'dir' to radians\n",
    "        (pl.col(\"o\") * np.pi / 180).alias(\"o_rad\"),\n",
    "        (pl.col(\"dir\") * np.pi / 180).alias(\"dir_rad\")\n",
    "    ]).with_columns([\n",
    "        # Pre-calculate shifted values\n",
    "        pl.col(\"x\").shift(1).over(\"PlayKey\").alias(\"prev_x\")\n",
    "        , pl.col(\"y\").shift(1).over(\"PlayKey\").alias(\"prev_y\")\n",
    "        # , pl.col(\"time\").shift(1).over(\"PlayKey\").alias(\"prev_time\")\n",
    "        , pl.col(\"dir_rad\").shift(1).over(\"PlayKey\").alias(\"prev_dir\")\n",
    "        , pl.col(\"o_rad\").shift(1).over(\"PlayKey\").alias(\"prev_o\")\n",
    "    ]).with_columns([\n",
    "        # Calculate the component displacements \n",
    "          (pl.col(\"x\") - pl.col(\"prev_x\")).alias(\"dx\")\n",
    "        , (pl.col(\"y\") - pl.col(\"prev_y\")).alias(\"dy\")\n",
    "    ]).with_columns([\n",
    "        # Calculate displacement\n",
    "        ((pl.col(\"dx\")**2 + pl.col(\"dy\")**2)**0.5).alias(\"Displacement\")\n",
    "    ]).with_columns([\n",
    "        # Calculate speed\n",
    "        (pl.col(\"Displacement\") / 0.1).alias(\"Speed\")\n",
    "        # Calculate direction\n",
    "        , (np.degrees(np.arctan2(pl.col(\"dx\"), pl.col(\"dy\")))).alias(\"Direction\")\n",
    "        # Calculate velocity components\n",
    "        , (pl.col(\"dx\") / 0.1).alias(\"vx\")\n",
    "        , (pl.col(\"dy\") / 0.1).alias(\"vy\")\n",
    "        # Calculate angular velocities\n",
    "        , ((pl.col(\"dir_rad\") - pl.col(\"prev_dir\")) / 0.1).alias(\"omega_dir\")\n",
    "        , ((pl.col(\"o_rad\") - pl.col(\"prev_o\")) / 0.1).alias(\"omega_o\")\n",
    "    ]).with_columns([\n",
    "        ((pl.col(\"omega_dir\") - pl.col(\"omega_o\")).abs()).alias(\"omega_diff\")\n",
    "    ]).drop([\n",
    "        \"prev_x\", \"prev_y\", \"prev_dir\", \"prev_o\", \"dx\", \"dy\", \"o_rad\", \"dir_rad\"\n",
    "    ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def impulse_calculator(df):\n",
    "    import numpy as np # type: ignore\n",
    "    import polars as pl # type: ignore\n",
    "    \"\"\"\n",
    "    Using the (X,Y) and time columns, perform calculations based on the velocities and changes \n",
    "    in velocites along with player mass to get the momentum and impulse, a measure that can \n",
    "    be assessed along with medical data related to concussions and injuries\n",
    "    \"\"\"\n",
    "    \n",
    "    return df.with_columns([\n",
    "        # Calculate the linear momentum for each instant\n",
    "        (pl.col('vx') * pl.col('Weight_kg')).alias('px')\n",
    "        , (pl.col('vy') * pl.col('Weight_kg')).alias('py')\n",
    "\n",
    "        # Calculate the moment of inertia of a rotating upright body (1/12 mr^2)\n",
    "        , (1/12 * pl.col('Weight_kg') * (pl.col('Chest_rad_m')**2)).alias('moment')\n",
    "        \n",
    "        # Calculate the moment of inertia of the upper body turning upright with respect to waist (70% mass)\n",
    "        , (1/12 * (pl.col('Weight_kg')*0.7) * (pl.col('Chest_rad_m')**2)).alias('moment_upper')\n",
    "    \n",
    "    ]).with_columns([\n",
    "          # Calculate the magnitude of linear momentum\n",
    "        ((pl.col(\"px\")**2 + pl.col(\"py\")**2)**0.5).alias(\"p_magnitude\")\n",
    "        \n",
    "        # Calculate the angular momentum for the direction\n",
    "        , (pl.col('omega_dir')*pl.col('moment')).alias('L_dir')\n",
    "\n",
    "        # Calculate the angular momentum of the upper body with respect to lower\n",
    "        , (pl.col('omega_diff')*pl.col('moment_upper')).alias('L_diff')\n",
    "\n",
    "\n",
    "    ]).with_columns([\n",
    "        # Pre-calculate shifted values for linear and angular momenta\n",
    "        pl.col(\"px\").shift(1).over(\"PlayKey\").alias(\"prev_px\")\n",
    "        , pl.col(\"py\").shift(1).over(\"PlayKey\").alias(\"prev_py\")\n",
    "        , pl.col(\"L_dir\").shift(1).over(\"PlayKey\").alias(\"prev_L_dir\")\n",
    "        , pl.col(\"L_diff\").shift(1).over(\"PlayKey\").alias(\"prev_L_diff\")\n",
    "        \n",
    "    ]).with_columns([\n",
    "        # Calculate impulse, J, which is the change in linear momentum \n",
    "        ((pl.col(\"px\") - pl.col(\"prev_px\"))).alias(\"Jx\")\n",
    "        , ((pl.col(\"py\") - pl.col(\"prev_py\"))).alias(\"Jy\")\n",
    "        \n",
    "    ]).with_columns([\n",
    "          # Calculate the magnitude of linear momentum\n",
    "        ((pl.col(\"Jx\")**2 + pl.col(\"Jy\")**2)**0.5).alias(\"J_magnitude\")\n",
    "\n",
    "        # Calculate torque as the change in angular momentum L over the change in time\n",
    "        , (((pl.col(\"L_dir\") - pl.col(\"prev_L_dir\"))) / 0.1).alias(\"torque\")\n",
    "        , (((pl.col(\"L_diff\") - pl.col(\"prev_L_diff\"))) / 0.1).alias(\"torque_internal\")\n",
    "\n",
    "    ]).drop([\n",
    "        \"prev_L_dir\", \"prev_px\", \"prev_py\", \"prev_L_diff\"\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(dataset): \n",
    "    \"\"\"\n",
    "    Accepts the desired dataset string and opens the file as either a polars dataframe\n",
    "    or a lazyframe. Lazyloading is used for the larger tracking datasets. \n",
    "    \"\"\"\n",
    "    import polars as pl  # type: ignore\n",
    "    import numpy as np # type: ignore\n",
    "\n",
    "    valid_datasets = ['plays', 'injuries', 'role_data', 'punt_data', 'play_information', 'game_data', 'video_review', 'tracking', 'ngs', 'qualitative_injuries', 'qualitative_concussions']\n",
    "    if dataset not in valid_datasets: \n",
    "        raise ValueError(f\"Invalid dataset name '{dataset}'. Valid options are: {valid_datasets}\")\n",
    "\n",
    "    try:\n",
    "        # Injury Datasets\n",
    "        if dataset == 'plays':\n",
    "            PlayList_path = \"F:/Data/nfl-playing-surface-analytics/PlayList.csv\"\n",
    "            df = pl.read_csv(PlayList_path)\n",
    "        elif dataset == 'injuries':\n",
    "            InjuryRecord_path = \"F:/Data/nfl-playing-surface-analytics/InjuryRecord.csv\"\n",
    "            df = pl.read_csv(InjuryRecord_path)\n",
    "\n",
    "        # Concussion Datasets\n",
    "        elif dataset == 'role_data':\n",
    "            play_player_role_data_path = \"F:/Data/NFL-Punt-Analytics-Competition/play_player_role_data.csv\"\n",
    "            df = pl.read_csv(play_player_role_data_path)\n",
    "        elif dataset == 'punt_data':\n",
    "            player_punt_data_path = \"F:/Data/NFL-Punt-Analytics-Competition/player_punt_data.csv\"\n",
    "            df = pl.read_csv(player_punt_data_path)\n",
    "        elif dataset == 'play_information':\n",
    "            play_information_path = \"F:/Data/NFL-Punt-Analytics-Competition/play_information.csv\"\n",
    "            df = pl.read_csv(play_information_path)\n",
    "        elif dataset == 'game_data':\n",
    "            game_data_path = \"F:/Data/NFL-Punt-Analytics-Competition/game_data.csv\"\n",
    "            df = pl.read_csv(game_data_path)\n",
    "        elif dataset == 'video_review':\n",
    "            video_review_path = \"F:/Data/NFL-Punt-Analytics-Competition/video_review.csv\"\n",
    "            df = pl.read_csv(video_review_path)\n",
    "\n",
    "        # Qualitative Datasets\n",
    "        elif dataset == 'qualitative_injuries':\n",
    "            qi_path = \"F:/Data/Clean_Data/qualitative_injuries.parquet\"\n",
    "            df = pl.read_parquet(qi_path)\n",
    "        elif dataset == 'qualitative_concussions':\n",
    "            qc_path = \"F:/Data/Clean_Data/qualitative_concussions.parquet\"\n",
    "            df = pl.read_parquet(qc_path)\n",
    "\n",
    "        # Tracking Datasets\n",
    "        elif dataset == 'tracking':\n",
    "            tracking_path = \"F:/Data/nfl-playing-surface-analytics/PlayerTrackData.csv\"\n",
    "            df = pl.read_csv(tracking_path)\n",
    "        elif dataset == 'ngs':\n",
    "            df\n",
    "\n",
    "        return df\n",
    "    \n",
    "    except Exception as e: \n",
    "        print(f\"An error occurred while loading the dataset '{dataset}': {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "tracking_path = \"F:/Data/nfl-playing-surface-analytics/PlayerTrackData.csv\"\n",
    "lf = pl.scan_csv(tracking_path, infer_schema_length=10000, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lazy_data_shrinker(lf, verbose=True):\n",
    "    \"\"\"\n",
    "    Optimize memory usage of a Polars LazyFrame for both categorical and numeric data.\n",
    "    \"\"\"\n",
    "    import polars as pl\n",
    "    import numpy as np\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Starting lazy data shrinking process...\")\n",
    "\n",
    "    # Drop the 'event' column\n",
    "    lf = lf.drop(['event', 'dis', 's'])\n",
    "\n",
    "    # Collect schema to avoid repeated schema resolution\n",
    "    schema = lf.collect_schema()\n",
    "\n",
    "    for col, col_type in schema.items():\n",
    "        if col_type in [pl.Int8, pl.Int16, pl.Int32, pl.Int64, pl.Float32, pl.Float64]:\n",
    "            try:\n",
    "                # For numeric columns, we'll use statistics to determine the appropriate type\n",
    "                stats = lf.select([\n",
    "                    pl.col(col).min().alias(\"min\"),\n",
    "                    pl.col(col).max().alias(\"max\"),\n",
    "                    pl.col(col).null_count().alias(\"null_count\")\n",
    "                ]).collect()\n",
    "                \n",
    "                c_min, c_max, null_count = stats[0, \"min\"], stats[0, \"max\"], stats[0, \"null_count\"]\n",
    "\n",
    "                if col_type.is_integer():\n",
    "                    if c_min >= np.iinfo(np.int8).min and c_max <= np.iinfo(np.int8).max:\n",
    "                        lf = lf.with_columns(pl.col(col).cast(pl.Int8))\n",
    "                    elif c_min >= np.iinfo(np.int16).min and c_max <= np.iinfo(np.int16).max:\n",
    "                        lf = lf.with_columns(pl.col(col).cast(pl.Int16))\n",
    "                    elif c_min >= np.iinfo(np.int32).min and c_max <= np.iinfo(np.int32).max:\n",
    "                        lf = lf.with_columns(pl.col(col).cast(pl.Int32))\n",
    "                    else:\n",
    "                        lf = lf.with_columns(pl.col(col).cast(pl.Int64))\n",
    "                elif col_type.is_float():\n",
    "                    lf = lf.with_columns(pl.col(col).cast(pl.Float64))\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing column {col}: {str(e)}\")\n",
    "                print(f\"Keeping original data type for column {col}\")\n",
    "\n",
    "        elif col_type == pl.Utf8:\n",
    "            if col != \"PlayKey\":\n",
    "                try:\n",
    "                    # For string columns, we'll check the cardinality to decide if it should be categorical\n",
    "                    unique_count = lf.select(pl.col(col).n_unique()).collect()[0, 0]\n",
    "                    total_count = lf.select(pl.len()).collect()[0, 0]\n",
    "                    if unique_count / total_count < 0.5:  # If less than 50% unique values\n",
    "                        lf = lf.with_columns(pl.col(col).cast(pl.Categorical))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing column {col}: {str(e)}\")\n",
    "                    print(f\"Keeping original data type for column {col}\")\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Lazy data shrinking process completed.\")\n",
    "\n",
    "    return lf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting lazy data shrinking process...\n",
      "Lazy data shrinking process completed.\n"
     ]
    }
   ],
   "source": [
    "optimized_lf = lazy_data_shrinker(lf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
