{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract and Transform the Injury and Concussion Tracking Data\n",
    "\n",
    "This data is much more voluminous than the qualitative data. Maintaining the datatypes is paramount to keeping the size smaller. Again, this will be the set of functions for all of the transformation processing of the tracking data and the listing of the .py files used to actually process and save the data. \n",
    "\n",
    "I think that I can process all of the data as a dataframe for most of the processing. There were issues in collecting the dataframe from a lazyframe following the impulse calculations, so I am foregoing that attempt and going back to what was already working. \n",
    "\n",
    "Since the summary table uses aggregates, I may be able to aggregate within a lazyframe, but it may not be so expensive that I can just do this as a dataframe as well, since this worked before. Since I will be performing the aggregation after the other calcualtions are performed, there is no reason not to do this in chunks. \n",
    "\n",
    "The functions that need to be performed are as follows: \n",
    "\n",
    "- Data Shrinker\n",
    "The optimized data should be saved as a parquet file for the whole dataset. Once collected, this datset can be broken into chunks, making sure that it is not divided between PlayKey. This WILL be critical, since some of the calculations are based on a time of zero and iterate through. \n",
    "\n",
    "- Angle Corrector\n",
    "- Velocity Calculator\n",
    "- Body Builder\n",
    "- Impulse Calculator\n",
    "\n",
    "Following these four functions, each chunk should be saved as a parquet file. \n",
    "\n",
    "In order to create the Quantitative Set, I will need to concatenate each of these files into one. However, since I can use these as lazyframes, I can utilize the parallelization of aggregate functions when processing during the collect(). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "\n",
    "injury_tracking_path = \"F:/Data/nfl-playing-surface-analytics/PlayerTrackData.csv\"\n",
    "optimized_path = \"F:/Data/Processing_data/OptimizedTrackData.parquet\"\n",
    "output_dir = \"F:/Data/Processing_data/injury_output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_angle_difference(angle1, angle2):\n",
    "    \"\"\"\n",
    "    Calculate the smallest angle difference between two angles \n",
    "    using trigonometric functions, accounting for edge cases.\n",
    "    \"\"\"\n",
    "    import numpy as np # type: ignore\n",
    "\n",
    "    sin_diff = np.sin(np.radians(angle2 - angle1))\n",
    "    cos_diff = np.cos(np.radians(angle2 - angle1))\n",
    "    return np.degrees(np.arctan2(sin_diff, cos_diff))\n",
    "\n",
    "def angle_corrector(df):\n",
    "    \"\"\"\n",
    "    Make corrections to angles to reduce fringe errors at 360\n",
    "    \"\"\"\n",
    "    import polars as pl # type: ignore\n",
    "\n",
    "    try: \n",
    "        df = df.with_columns([\n",
    "            ((pl.col(\"dir\") + 180) % 360 - 180).alias(\"dir\")\n",
    "            , ((pl.col(\"o\") + 180) % 360 - 180).alias(\"o\")\n",
    "        ]).with_columns(\n",
    "            (calculate_angle_difference(pl.col(\"dir\"), pl.col(\"o\"))).abs().round(2).alias(\"Angle_Diff\")\n",
    "            )\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except Exception as e: \n",
    "        print(f\"An error occurred during calculate_angle_difference: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def velocity_calculator(df):\n",
    "    \"\"\"\n",
    "    Using the (X,Y) and time columns, perform calculations based on the \n",
    "    difference between two rows to find displacement, speed, direction \n",
    "    of motion, velocity in x and y components, and the angular velocities \n",
    "    of the direction of motion and orientations \n",
    "    \"\"\"\n",
    "    import numpy as np # type: ignore\n",
    "    import polars as pl # type: ignore\n",
    "\n",
    "    try:\n",
    "        return df.with_columns([\n",
    "            # Convert 'o' and 'dir' to radians\n",
    "            (pl.col(\"o\") * np.pi / 180).alias(\"o_rad\"),\n",
    "            (pl.col(\"dir\") * np.pi / 180).alias(\"dir_rad\")\n",
    "        ]).with_columns([\n",
    "            # Pre-calculate shifted values\n",
    "            pl.col(\"x\").shift(1).over(\"PlayKey\").alias(\"prev_x\")\n",
    "            , pl.col(\"y\").shift(1).over(\"PlayKey\").alias(\"prev_y\")\n",
    "            # , pl.col(\"time\").shift(1).over(\"PlayKey\").alias(\"prev_time\")\n",
    "            , pl.col(\"dir_rad\").shift(1).over(\"PlayKey\").alias(\"prev_dir\")\n",
    "            , pl.col(\"o_rad\").shift(1).over(\"PlayKey\").alias(\"prev_o\")\n",
    "        ]).with_columns([\n",
    "            # Calculate the component displacements \n",
    "            (pl.col(\"x\") - pl.col(\"prev_x\")).alias(\"dx\")\n",
    "            , (pl.col(\"y\") - pl.col(\"prev_y\")).alias(\"dy\")\n",
    "        ]).with_columns([\n",
    "            # Calculate displacement\n",
    "            ((pl.col(\"dx\")**2 + pl.col(\"dy\")**2)**0.5).alias(\"Displacement\")\n",
    "        ]).with_columns([\n",
    "            # Calculate speed\n",
    "            (pl.col(\"Displacement\") / 0.1).alias(\"Speed\")\n",
    "            # Calculate direction\n",
    "            , (np.degrees(np.arctan2(pl.col(\"dx\"), pl.col(\"dy\")))).alias(\"Direction\")\n",
    "            # Calculate velocity components\n",
    "            , (pl.col(\"dx\") / 0.1).alias(\"vx\")\n",
    "            , (pl.col(\"dy\") / 0.1).alias(\"vy\")\n",
    "            # Calculate angular velocities\n",
    "            , ((pl.col(\"dir_rad\") - pl.col(\"prev_dir\")) / 0.1).alias(\"omega_dir\")\n",
    "            , ((pl.col(\"o_rad\") - pl.col(\"prev_o\")) / 0.1).alias(\"omega_o\")\n",
    "        ]).with_columns([\n",
    "            ((pl.col(\"omega_dir\") - pl.col(\"omega_o\")).abs()).alias(\"omega_diff\")\n",
    "        ]).drop([\n",
    "            \"prev_x\", \"prev_y\", \"prev_dir\", \"prev_o\", \"dx\", \"dy\", \"o_rad\", \"dir_rad\"\n",
    "        ])\n",
    "    \n",
    "    except Exception as e: \n",
    "        print(f\"An error occurred during velocity_calculator: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def body_builder(df):\n",
    "    \"\"\"\n",
    "    This uses averages collected for height, weight, and chest radius for each position. This information\n",
    "    is used to determine the momentum and impulse rather than just looking at velocities in the analysis. Chest\n",
    "    radius is needed for angular moment of inertia as a rotating cyliner.\n",
    "    \"\"\"\n",
    "    import polars as pl # type: ignore\n",
    "\n",
    "    try: \n",
    "        body_data = pl.LazyFrame({\n",
    "            \"Position\": [\"QB\", \"RB\", \"FB\", \"WR\", \"TE\", \"T\", \"G\", \"C\", \"DE\", \"DT\", \"NT\", \"LB\", \"OLB\", \"MLB\", \"CB\", \"S\", \"K\", \"P\", \"SS\", \"ILB\", \"FS\", \"LS\", \"DB\"]\n",
    "            # , \"Position_Name\": [\"Quarterback\", \"Running Back\", \"Fullback\", \"Wide Receiver\", \"Tight End\", \"Tackle\", \"Guard\", \"Center\", \"Defensive End\", \"Defensive Tackle\", \"Nose Tackle\", \"Linebacker\", \"Outside Linebacker\", \"Middle Linebacker\", \"Cornerback\", \"Safety\", \"Kicker\", \"Punter\", \"Strong Safety\", \"Inside Linebacker\", \"Free Safety\", \"Long Snapper\", \"Defensive Back\"]\n",
    "            , \"Height_m\": [1.91, 1.79, 1.85, 1.88, 1.96, 1.97, 1.90, 1.87, 1.97, 1.92, 1.88, 1.90, 1.90, 1.87, 1.82, 1.84, 1.83, 1.88, 1.84, 1.90, 1.84, 1.88, 1.82]\n",
    "            , \"Weight_kg\": [102.1, 95.3, 111.1, 90.7, 114.6, 140.6, 141.8, 136.1, 120.2, 141.8, 152.0, 110.0, 108.9, 113.4, 87.4, 95.9, 92.08, 97.52, 95.9, 110.0, 95.9, 108.86, 87.4]\n",
    "            , \"Chest_rad_m\": [0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191]\n",
    "            })\n",
    "\n",
    "        PlayList_path = \"F:/Data/nfl-playing-surface-analytics/PlayList.csv\"\n",
    "        position = pl.scan_csv(PlayList_path).select([\"PlayKey\", \"Position\"])\n",
    "        position = position.join(\n",
    "                body_data\n",
    "                , on='Position'\n",
    "                , how='left'\n",
    "                )\n",
    "\n",
    "        df = df.join(\n",
    "                position\n",
    "                , on='PlayKey'\n",
    "                , how='left'\n",
    "            )    \n",
    "        \n",
    "\n",
    "        return df.filter(pl.col('Position').is_not_null())    \n",
    "        \n",
    "    except Exception as e: \n",
    "        print(f\"An error occurred during body_builder: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impulse_calculator(df):\n",
    "    \"\"\"\n",
    "    Using the (X,Y) and time columns, perform calculations based on the velocities and changes \n",
    "    in velocites along with player mass to get the momentum and impulse, a measure that can \n",
    "    be assessed along with medical data related to concussions and injuries\n",
    "    \"\"\"\n",
    "    import numpy as np # type: ignore\n",
    "    import polars as pl # type: ignore\n",
    "    \n",
    "\n",
    "    try: \n",
    "        return df.with_columns([\n",
    "            # Calculate the linear momentum for each instant\n",
    "            (pl.col('vx') * pl.col('Weight_kg')).alias('px')\n",
    "            , (pl.col('vy') * pl.col('Weight_kg')).alias('py')\n",
    "\n",
    "            # Calculate the moment of inertia of a rotating upright body (1/12 mr^2)\n",
    "            , (1/12 * pl.col('Weight_kg') * (pl.col('Chest_rad_m')**2)).alias('moment')\n",
    "            \n",
    "            # Calculate the moment of inertia of the upper body turning upright with respect to waist (70% mass)\n",
    "            , (1/12 * (pl.col('Weight_kg')*0.7) * (pl.col('Chest_rad_m')**2)).alias('moment_upper')\n",
    "        \n",
    "        ]).with_columns([\n",
    "            # Calculate the magnitude of linear momentum\n",
    "            ((pl.col(\"px\")**2 + pl.col(\"py\")**2)**0.5).alias(\"p_magnitude\")\n",
    "            \n",
    "            # Calculate the angular momentum for the direction\n",
    "            , (pl.col('omega_dir')*pl.col('moment')).alias('L_dir')\n",
    "\n",
    "            # Calculate the angular momentum of the upper body with respect to lower\n",
    "            , (pl.col('omega_diff')*pl.col('moment_upper')).alias('L_diff')\n",
    "\n",
    "\n",
    "        ]).with_columns([\n",
    "            # Pre-calculate shifted values for linear and angular momenta\n",
    "            pl.col(\"px\").shift(1).over(\"PlayKey\").alias(\"prev_px\")\n",
    "            , pl.col(\"py\").shift(1).over(\"PlayKey\").alias(\"prev_py\")\n",
    "            , pl.col(\"L_dir\").shift(1).over(\"PlayKey\").alias(\"prev_L_dir\")\n",
    "            , pl.col(\"L_diff\").shift(1).over(\"PlayKey\").alias(\"prev_L_diff\")\n",
    "            \n",
    "        ]).with_columns([\n",
    "            # Calculate impulse, J, which is the change in linear momentum \n",
    "            ((pl.col(\"px\") - pl.col(\"prev_px\"))).alias(\"Jx\")\n",
    "            , ((pl.col(\"py\") - pl.col(\"prev_py\"))).alias(\"Jy\")\n",
    "            \n",
    "        ]).with_columns([\n",
    "            # Calculate the magnitude of linear momentum\n",
    "            ((pl.col(\"Jx\")**2 + pl.col(\"Jy\")**2)**0.5).alias(\"J_magnitude\")\n",
    "\n",
    "            # Calculate torque as the change in angular momentum L over the change in time\n",
    "            , (((pl.col(\"L_dir\") - pl.col(\"prev_L_dir\"))) / 0.1).alias(\"torque\")\n",
    "            , (((pl.col(\"L_diff\") - pl.col(\"prev_L_diff\"))) / 0.1).alias(\"torque_internal\")\n",
    "\n",
    "        ]).drop([\n",
    "            \"prev_L_dir\", \"prev_px\", \"prev_py\", \"prev_L_diff\"\n",
    "        ])\n",
    "    \n",
    "    except Exception as e: \n",
    "        print(f\"An error occurred during the impulse_calculator, which surprises no one.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_initial_lazyframe(injury_tracking_path):\n",
    "    return pl.scan_csv(injury_tracking_path, truncate_ragged_lines=True, infer_schema_length=10000, ignore_errors=True).drop(['event', 's', 'dis'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_shrinker(df, verbose=True):\n",
    "    \"\"\"\n",
    "    Optimize memory usage of a Polars dataframe for both categorical and numeric data.\n",
    "    \"\"\"\n",
    "    import polars as pl # type: ignore\n",
    "    import numpy as np # type: ignore\n",
    "\n",
    "    start_mem = df.estimated_size(\"mb\")\n",
    "    if verbose:\n",
    "        print(f'Memory usage of dataframe is {start_mem:.2f} MB')\n",
    "\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type in [pl.Int8, pl.Int16, pl.Int32, pl.Int64, pl.Float32, pl.Float64]:\n",
    "            # Handle missing values\n",
    "            if df[col].null_count() > 0:\n",
    "                c_min = df[col].min() if df[col].min() is not None else float('nan')\n",
    "                c_max = df[col].max() if df[col].max() is not None else float('nan')\n",
    "            else:\n",
    "                c_min = df[col].min()\n",
    "                c_max = df[col].max()\n",
    "\n",
    "            if col_type.is_integer():\n",
    "                if c_min >= np.iinfo(np.int8).min and c_max <= np.iinfo(np.int8).max:\n",
    "                    df = df.with_columns(pl.col(col).cast(pl.Int8))\n",
    "                elif c_min >= np.iinfo(np.int16).min and c_max <= np.iinfo(np.int16).max:\n",
    "                    df = df.with_columns(pl.col(col).cast(pl.Int16))\n",
    "                elif c_min >= np.iinfo(np.int32).min and c_max <= np.iinfo(np.int32).max:\n",
    "                    df = df.with_columns(pl.col(col).cast(pl.Int32))\n",
    "                else:\n",
    "                    df = df.with_columns(pl.col(col).cast(pl.Int64))\n",
    "            else:\n",
    "                if c_min >= np.finfo(np.float32).min and c_max <= np.finfo(np.float32).max:\n",
    "                    df = df.with_columns(pl.col(col).cast(pl.Float32))\n",
    "                else:\n",
    "                    df = df.with_columns(pl.col(col).cast(pl.Float64))\n",
    "\n",
    "        elif col_type == pl.Utf8:\n",
    "            if col != \"PlayKey\" and df[col].n_unique() / len(df) < 0.5:  # If less than 50% unique values\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Categorical))\n",
    "\n",
    "    end_mem = df.estimated_size(\"mb\")\n",
    "\n",
    "    optimized_schema = df.schema\n",
    "\n",
    "    if verbose:\n",
    "        print(f'Memory usage after optimization is: {end_mem:.2f} MB')\n",
    "        print(f'Decreased by {100 * (start_mem - end_mem) / start_mem:.1f}%')\n",
    "\n",
    "    return df, optimized_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_and_save_lazyframe(injury_tracking_path, optimized_path):\n",
    "    \"\"\"\n",
    "    This function opens the original tracking csv, reduces size by casting to less expensive data types, \n",
    "    and then saves the optimized tracking dataset to file. It does not return a dataframe, so any use will \n",
    "    be expected to call from the saved optimized file. \n",
    "    \"\"\"\n",
    "    import polars as pl\n",
    "\n",
    "    df = create_initial_lazyframe(injury_tracking_path).collect(streaming=True)\n",
    "    optimized_df, optimized_schema = data_shrinker(df)\n",
    "    \n",
    "    # Cast the DataFrame columns to the types specified in optimized_schema\n",
    "    for column, dtype in optimized_schema.items():\n",
    "        optimized_df = optimized_df.with_columns(pl.col(column).cast(dtype))\n",
    "\n",
    "    # Write the DataFrame to a Parquet file\n",
    "    optimized_df.write_parquet(optimized_path)\n",
    "\n",
    "    print(f\"Saved optimized data to {optimized_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 3674.06 MB\n",
      "Memory usage after optimization is: 2217.48 MB\n",
      "Decreased by 39.6%\n",
      "Saved optimized data to F:/Data/Processing_data/OptimizedTrackData.parquet\n"
     ]
    }
   ],
   "source": [
    "optimize_and_save_lazyframe(injury_tracking_path, optimized_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, the optimized df has been written as a parquet file. I returned this file and opened the new saved file to verify that the save is maintaining the reduced data structure, and this has been verified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.read_parquet(optimized_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def lazy_data_shrinker(lf, verbose=True):\n",
    "#     \"\"\"\n",
    "#     Optimize memory usage of a Polars LazyFrame for both categorical and numeric data.\n",
    "#     \"\"\"\n",
    "#     import polars as pl\n",
    "#     import numpy as np\n",
    "\n",
    "#     if verbose:\n",
    "#         print(\"Starting lazy data shrinking process...\")\n",
    "\n",
    "#     # Collect schema to avoid repeated schema resolution\n",
    "#     schema = lf.collect_schema()\n",
    "\n",
    "#     for col, col_type in schema.items():\n",
    "#         if col_type in [pl.Int8, pl.Int16, pl.Int32, pl.Int64, pl.Float32, pl.Float64]:\n",
    "#             try:\n",
    "#                 # For numeric columns, we'll use statistics to determine the appropriate type\n",
    "#                 stats = lf.select([\n",
    "#                     pl.col(col).min().alias(\"min\"),\n",
    "#                     pl.col(col).max().alias(\"max\"),\n",
    "#                     pl.col(col).null_count().alias(\"null_count\")\n",
    "#                 ]).collect()\n",
    "                \n",
    "#                 c_min, c_max, null_count = stats[0, \"min\"], stats[0, \"max\"], stats[0, \"null_count\"]\n",
    "\n",
    "#                 if col_type.is_integer():\n",
    "#                     if c_min >= np.iinfo(np.int8).min and c_max <= np.iinfo(np.int8).max:\n",
    "#                         lf = lf.with_columns(pl.col(col).cast(pl.Int8))\n",
    "#                     elif c_min >= np.iinfo(np.int16).min and c_max <= np.iinfo(np.int16).max:\n",
    "#                         lf = lf.with_columns(pl.col(col).cast(pl.Int16))\n",
    "#                     elif c_min >= np.iinfo(np.int32).min and c_max <= np.iinfo(np.int32).max:\n",
    "#                         lf = lf.with_columns(pl.col(col).cast(pl.Int32))\n",
    "#                     else:\n",
    "#                         lf = lf.with_columns(pl.col(col).cast(pl.Int64))\n",
    "#                 elif col_type.is_float():\n",
    "#                     lf = lf.with_columns(pl.col(col).cast(pl.Float64))\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error processing column {col}: {str(e)}\")\n",
    "#                 print(f\"Keeping original data type for column {col}\")\n",
    "\n",
    "#         elif col_type == pl.Utf8:\n",
    "#             if col != \"PlayKey\":\n",
    "#                 try:\n",
    "#                     # For string columns, we'll check the cardinality to decide if it should be categorical\n",
    "#                     unique_count = lf.select(pl.col(col).n_unique()).collect()[0, 0]\n",
    "#                     total_count = lf.select(pl.len()).collect()[0, 0]\n",
    "#                     if unique_count / total_count < 0.5:  # If less than 50% unique values\n",
    "#                         lf = lf.with_columns(pl.col(col).cast(pl.Categorical))\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Error processing column {col}: {str(e)}\")\n",
    "#                     print(f\"Keeping original data type for column {col}\")\n",
    "\n",
    "#     if verbose:\n",
    "#         print(\"Lazy data shrinking process completed.\")\n",
    "\n",
    "#     optimized_schema = lf.schema\n",
    "\n",
    "#     return lf, optimized_schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the optimization to save the dataframe\n",
    "optimize_and_save_lazyframe(injury_tracking_path=injury_tracking_path, optimized_path=optimized_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_save_playkey_group(lazy_df, playkeys, output_dir, group_number):\n",
    "    # Filter the lazy DataFrame for the specific PlayKeys\n",
    "    group_df = lazy_df.filter(pl.col(\"PlayKey\").is_in(playkeys))\n",
    "    \n",
    "    # Processing\n",
    "    group_df = (group_df\n",
    "                .pipe(angle_corrector)\n",
    "                .pipe(velocity_calculator)\n",
    "                .pipe(body_builder)\n",
    "                .pipe(impulse_calculator))\n",
    "\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Save the DataFrame for this group as a Parquet file\n",
    "    output_file = os.path.join(output_dir, f\"group_{group_number}.parquet\")\n",
    "    group_df.collect().write_parquet(output_file)\n",
    "    print(f\"Saved data for PlayKey group: {group_number}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(optimized_path, output_dir, group_size=20000):\n",
    "    import math\n",
    "    \n",
    "    # Scan the Parquet file (which is already an optimized LazyFrame)\n",
    "    lazy_df = pl.scan_parquet(optimized_path)\n",
    "    \n",
    "    # Get unique PlayKey values\n",
    "    unique_playkeys = lazy_df.select(pl.col(\"PlayKey\").unique()).collect()[\"PlayKey\"].to_list()\n",
    "    \n",
    "    # Calculate the number of groups\n",
    "    num_groups = math.ceil(len(unique_playkeys) / group_size)\n",
    "    \n",
    "    # Process each group of PlayKeys\n",
    "    for i in range(num_groups):\n",
    "        start_idx = i * group_size\n",
    "        end_idx = min((i + 1) * group_size, len(unique_playkeys))\n",
    "        playkey_group = unique_playkeys[start_idx:end_idx]\n",
    "        process_and_save_playkey_group(lazy_df, playkey_group, output_dir, i + 1)\n",
    "\n",
    "    print(\"Processing complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_injury_data(optimized_path, output_dir):\n",
    "    \"\"\"\n",
    "    Full transform process for the surface injury data.\n",
    "    Output options are for returning a summary df to the database or the full tracking with \n",
    "    additional columns added\n",
    "    \"\"\"\n",
    "\n",
    "    if __name__ == \"__main__\":\n",
    "        process_file(optimized_path, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    transform_injury_data(optimized_path=optimized_path, output_dir=output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.read_parquet(\"F:/Data/Processing_data/output/group_1.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_optimized = data_shrinker(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_optimized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Summary table, I don't need all of these parameters. I will only need the following: \n",
    "\n",
    "- Playkey\n",
    "- Displacement\n",
    "- x\n",
    "- y\n",
    "- Angle_Diff\n",
    "- Speed\n",
    "- J_magnitude\n",
    "- torque\n",
    "- torque_internal\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
